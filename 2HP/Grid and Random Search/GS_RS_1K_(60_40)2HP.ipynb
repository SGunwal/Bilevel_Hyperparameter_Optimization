{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7gPDJoxpV7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er03ye7gpa0h"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXnVjHOPpax8",
        "outputId": "b804bcdd-2b0a-455b-eb75-eabd917aea2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600, 784), (600,), (400, 784), (400,), (10010, 784), (10010,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train_dataset = np.load('train_dataset1_60.npz')\n",
        "val_dataset = np.load('validation_dataset1_40.npz')\n",
        "test_dataset = np.load('test_dataset.npz')\n",
        "\n",
        "# =========== Loading Datasets ===============\n",
        "\n",
        "x_train = train_dataset['x'].reshape(600, 784).astype(\"float32\") / 255\n",
        "y_train = train_dataset['y'].astype(\"float32\")\n",
        "  \n",
        "x_val = val_dataset['x'].reshape(400, 784).astype(\"float32\") / 255\n",
        "y_val = val_dataset['y'].astype(\"float32\")   \n",
        "                    \n",
        "x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n",
        "y_test = test_dataset['y'].astype(\"float32\")                    \n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TgIaOgCpavt"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZiEYsk4pate"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits): # Lambda\n",
        "    total_loss = loss_fn(y_train,logits)\n",
        "    return total_loss + (tf.math.exp(lamda1)*tf.nn.l2_loss(w1) + tf.math.exp(lamda2)*tf.nn.l2_loss(w2))/(2*y_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_ucMfCjparJ"
      },
      "outputs": [],
      "source": [
        "wt_layer1_init = model.layers[1].get_weights()\n",
        "wt_layer2_init = model.layers[2].get_weights()\n",
        "losses = []\n",
        "train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "train_df = train_df.shuffle(buffer_size = 1024).batch(64)\n",
        "\n",
        "def fmin_loss(lamda1, lamda2, l_rate, momentum, epochs= 50, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    tf.keras.backend.clear_session()\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n",
        "    total_loss0 = 1e20\n",
        "    losses.clear()\n",
        "    for epoch in range(epochs):\n",
        "        for step,(x_train,y_train) in enumerate(train_df):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_train, training=True)\n",
        "                w1 = model.layers[1].weights[0]\n",
        "                w2 = model.layers[2].weights[0]\n",
        "                total_loss1 = loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits)\n",
        "                \n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "        total_loss0 = total_loss1\n",
        "        losses.append(total_loss0)\n",
        "    wt_layer1 = model.layers[1].get_weights()\n",
        "    wt_layer2 = model.layers[2].get_weights()\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "\n",
        "    return [total_loss1, wt_layer1, wt_layer2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuaCUx7fVJ8B"
      },
      "outputs": [],
      "source": [
        "l_rates = np.linspace(0.1,0.9,9)\n",
        "momentas = np.arange(0.01,1,0.01,dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arwRLnxJpaot"
      },
      "outputs": [],
      "source": [
        "# fmin_loss(-1.3793104, -0.3448276,0.5,0.0799)\n",
        "# mn_loss = 1e12\n",
        "# glbl_l_rate = None\n",
        "# glbl_momentum = None\n",
        "# for l_rate in l_rates:\n",
        "#   for momentum in momentas:\n",
        "#     model.layers[1].set_weights(wt_layer1_init)\n",
        "#     model.layers[2].set_weights(wt_layer2_init)\n",
        "#     loss_ , _ , _ = fmin_loss(-5.0,l_rate,momentum)\n",
        "#     print(f'loss = {loss_} , l_rate = {l_rate}, momentum = {momentum}')\n",
        "#     if loss_ < mn_loss:\n",
        "#       glbl_l_rate = l_rate\n",
        "#       glbl_momentum = momentum\n",
        "#       mn_loss = loss_\n",
        "\n",
        "# print('###############################################################')\n",
        "# print('--------------------Result-------------------------------------')\n",
        "# print(f'final_loss = {mn_loss} , l_rate = {glbl_l_rate} , glbl_momentum = {glbl_momentum}')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuWUkliCUiv8"
      },
      "outputs": [],
      "source": [
        "# y = losses\n",
        "# x = [i+1 for i in range(200)]\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(x,y)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeDP-YuhUQLH"
      },
      "outputs": [],
      "source": [
        "# y = losses\n",
        "# x = [i+1 for i in range(100)]\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(x,y)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd-VRozwT0S6"
      },
      "outputs": [],
      "source": [
        "# y = losses\n",
        "# x = [i+1 for i in range(50)]\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.plot(x,y)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH5E8bq3X-Fy"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyvSKtZ-palY"
      },
      "outputs": [],
      "source": [
        "lamdas = np.linspace(-10,0,30, dtype='float32')\n",
        "lamda_grid = [[i,j] for i in lamdas for j in lamdas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNhDh6Mpai7",
        "outputId": "c929ec54-e1de-48a9-f0aa-e0a0253c7ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min  loss at lamda = [-10.0, -10.0] is train_loss = 0.0039035603404045105, val_loss = 0.5539500713348389 , test_loss = 0.5245660543441772\n",
            "min  loss at lamda = [-10.0, -8.620689] is train_loss = 0.003770330687984824, val_loss = 0.5439983606338501 , test_loss = 0.5111501216888428\n",
            "min  loss at lamda = [-10.0, -7.586207] is train_loss = 0.003893473884090781, val_loss = 0.5307661294937134 , test_loss = 0.5100322365760803\n",
            "min  loss at lamda = [-10.0, -2.7586207] is train_loss = 0.005561762023717165, val_loss = 0.5217079520225525 , test_loss = 0.49437159299850464\n",
            "min  loss at lamda = [-10.0, -2.413793] is train_loss = 0.006065226625651121, val_loss = 0.5133768320083618 , test_loss = 0.485772043466568\n",
            "min  loss at lamda = [-10.0, -2.0689654] is train_loss = 0.007150200195610523, val_loss = 0.5125202536582947 , test_loss = 0.4823510944843292\n",
            "min  loss at lamda = [-10.0, -1.7241379] is train_loss = 0.008833025582134724, val_loss = 0.5044297575950623 , test_loss = 0.467305064201355\n",
            "min  loss at lamda = [-10.0, -1.3793104] is train_loss = 0.010887866839766502, val_loss = 0.4787447452545166 , test_loss = 0.45445677638053894\n",
            "min  loss at lamda = [-10.0, -1.0344827] is train_loss = 0.014459519647061825, val_loss = 0.4703806936740875 , test_loss = 0.45271095633506775\n",
            "min  loss at lamda = [-9.655172, -0.3448276] is train_loss = 0.024969426915049553, val_loss = 0.46173301339149475 , test_loss = 0.43532201647758484\n",
            "min  loss at lamda = [-9.655172, 0.0] is train_loss = 0.02961188368499279, val_loss = 0.45305347442626953 , test_loss = 0.4367474317550659\n",
            "min  loss at lamda = [-4.137931, -0.3448276] is train_loss = 0.02421809732913971, val_loss = 0.45009270310401917 , test_loss = 0.42503103613853455\n",
            "min  loss at lamda = [-2.7586207, 0.0] is train_loss = 0.034299734979867935, val_loss = 0.4407067894935608 , test_loss = 0.4152238368988037\n",
            "min  loss at lamda = [0.0, -3.4482758] is train_loss = 0.0332200787961483, val_loss = 0.43276405334472656 , test_loss = 0.4193294942378998\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [0.0, -3.4482758] is train_loss = 0.0332200787961483, val_loss = 0.43276405334472656 , test_loss = 0.4193294942378998\n"
          ]
        }
      ],
      "source": [
        "glbl_l_rate = 0.5\n",
        "glbl_momentum = 0.079\n",
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in lamda_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UveWFSMq8o8e"
      },
      "outputs": [],
      "source": [
        "random_lamda = np.random.random(30)     #value will be only between [0,1)\n",
        "ran_lambda = np.sort(10*random_lamda.astype('float32') - 10)\n",
        "ran_grid = [[i,j] for i in ran_lambda for j in ran_lambda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N90zo3Nc9Lk0",
        "outputId": "7016c31f-388e-488d-c702-f11cbe78c822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min  loss at lamda = [-9.947112, -9.947112] is train_loss = 0.003944482654333115, val_loss = 0.5591757297515869 , test_loss = 0.5257105827331543\n",
            "min  loss at lamda = [-9.947112, -9.633759] is train_loss = 0.003900391748175025, val_loss = 0.5552642941474915 , test_loss = 0.5231519341468811\n",
            "min  loss at lamda = [-9.947112, -8.802883] is train_loss = 0.0038315034471452236, val_loss = 0.5403742790222168 , test_loss = 0.5034715533256531\n",
            "min  loss at lamda = [-9.947112, -7.4025574] is train_loss = 0.0038341025356203318, val_loss = 0.5305639505386353 , test_loss = 0.5105301737785339\n",
            "min  loss at lamda = [-9.947112, -3.2799363] is train_loss = 0.004727335646748543, val_loss = 0.5179455280303955 , test_loss = 0.4954743981361389\n",
            "min  loss at lamda = [-9.947112, -2.0614963] is train_loss = 0.0072151185013353825, val_loss = 0.51263028383255 , test_loss = 0.48609867691993713\n",
            "min  loss at lamda = [-9.947112, -1.5578079] is train_loss = 0.009785624220967293, val_loss = 0.5083457231521606 , test_loss = 0.46370118856430054\n",
            "min  loss at lamda = [-9.947112, -0.92107296] is train_loss = 0.015409141778945923, val_loss = 0.48304682970046997 , test_loss = 0.45133841037750244\n",
            "min  loss at lamda = [-9.947112, -0.54832363] is train_loss = 0.01976456493139267, val_loss = 0.46139541268348694 , test_loss = 0.4302622675895691\n",
            "min  loss at lamda = [-9.947112, -0.39847946] is train_loss = 0.022584088146686554, val_loss = 0.45504218339920044 , test_loss = 0.4318295121192932\n",
            "min  loss at lamda = [-9.633759, -0.19830894] is train_loss = 0.033778320997953415, val_loss = 0.451941579580307 , test_loss = 0.43295472860336304\n",
            "min  loss at lamda = [-9.014237, -0.19830894] is train_loss = 0.0276603065431118, val_loss = 0.4477418065071106 , test_loss = 0.43625563383102417\n",
            "min  loss at lamda = [-1.5578079, -0.43000126] is train_loss = 0.032729241997003555, val_loss = 0.4452637732028961 , test_loss = 0.4194602370262146\n",
            "min  loss at lamda = [-0.54832363, -4.7880406] is train_loss = 0.02046484686434269, val_loss = 0.4403328001499176 , test_loss = 0.422762006521225\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [-0.54832363, -4.7880406] is train_loss = 0.02046484686434269, val_loss = 0.4403328001499176 , test_loss = 0.422762006521225\n"
          ]
        }
      ],
      "source": [
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in ran_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  \n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APBn3Tth9V8o"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "old_GS_RS_1K_(60_40)2HP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}