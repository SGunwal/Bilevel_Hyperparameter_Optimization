{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7gPDJoxpV7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er03ye7gpa0h"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXnVjHOPpax8",
        "outputId": "18720976-f392-4e09-82b8-642db0251667"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3000, 784), (3000,), (2000, 784), (2000,), (10010, 784), (10010,))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = np.load('train_dataset_5k_60.npz')\n",
        "val_dataset = np.load('validation_dataset_5k_40.npz')\n",
        "test_dataset = np.load('test_dataset.npz')\n",
        "\n",
        "# =========== Loading Datasets ===============\n",
        "\n",
        "x_train = train_dataset['x'].reshape(3000, 784).astype(\"float32\") / 255\n",
        "y_train = train_dataset['y'].astype(\"float32\")\n",
        "  \n",
        "x_val = val_dataset['x'].reshape(2000, 784).astype(\"float32\") / 255\n",
        "y_val = val_dataset['y'].astype(\"float32\")   \n",
        "                    \n",
        "x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n",
        "y_test = test_dataset['y'].astype(\"float32\")                    \n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TgIaOgCpavt"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n",
        "])\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZiEYsk4pate"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits): # Lambda\n",
        "    total_loss = loss_fn(y_train,logits)\n",
        "    return total_loss + (tf.math.exp(lamda1)*tf.nn.l2_loss(w1) + tf.math.exp(lamda2)*tf.nn.l2_loss(w2))/(2*y_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_ucMfCjparJ"
      },
      "outputs": [],
      "source": [
        "wt_layer1_init = model.layers[1].get_weights()\n",
        "wt_layer2_init = model.layers[2].get_weights()\n",
        "\n",
        "train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "train_df = train_df.shuffle(buffer_size = 1024).batch(64)\n",
        "\n",
        "def fmin_loss(lamda1, lamda2, l_rate, momentum, epochs= 50, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    tf.keras.backend.clear_session()\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n",
        "    total_loss0 = 1e20\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step,(x_train,y_train) in enumerate(train_df):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_train, training=True)\n",
        "                w1 = model.layers[1].weights[0]\n",
        "                w2 = model.layers[2].weights[0]\n",
        "                total_loss1 = loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits)\n",
        "                \n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "        total_loss0 = total_loss1\n",
        "\n",
        "    wt_layer1 = model.layers[1].get_weights()\n",
        "    wt_layer2 = model.layers[2].get_weights()\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "\n",
        "    return [total_loss1, wt_layer1, wt_layer2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcpmk75yDD5T",
        "outputId": "41a209db-2c98-495a-fe88-27b2f6cf0023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss = 0.04412248358130455 , l_rate = 0.1, momentum = 0.009999999776482582\n",
            "loss = 0.027074240148067474 , l_rate = 0.1, momentum = 0.019999999552965164\n",
            "loss = 0.02286617085337639 , l_rate = 0.1, momentum = 0.029999999329447746\n",
            "loss = 0.051924463361501694 , l_rate = 0.1, momentum = 0.03999999910593033\n",
            "loss = 0.044988904148340225 , l_rate = 0.1, momentum = 0.04999999701976776\n",
            "loss = 0.04640210047364235 , l_rate = 0.1, momentum = 0.059999994933605194\n",
            "loss = 0.02805834449827671 , l_rate = 0.1, momentum = 0.07000000029802322\n",
            "loss = 0.047282151877880096 , l_rate = 0.1, momentum = 0.07999999821186066\n",
            "loss = 0.03773213177919388 , l_rate = 0.1, momentum = 0.08999999612569809\n",
            "loss = 0.02652968466281891 , l_rate = 0.2, momentum = 0.009999999776482582\n",
            "loss = 0.022068822756409645 , l_rate = 0.2, momentum = 0.019999999552965164\n",
            "loss = 0.020238641649484634 , l_rate = 0.2, momentum = 0.029999999329447746\n",
            "loss = 0.024280274286866188 , l_rate = 0.2, momentum = 0.03999999910593033\n",
            "loss = 0.02051091007888317 , l_rate = 0.2, momentum = 0.04999999701976776\n",
            "loss = 0.025448467582464218 , l_rate = 0.2, momentum = 0.059999994933605194\n",
            "loss = 0.02301143668591976 , l_rate = 0.2, momentum = 0.07000000029802322\n",
            "loss = 0.02197791263461113 , l_rate = 0.2, momentum = 0.07999999821186066\n",
            "loss = 0.02473367005586624 , l_rate = 0.2, momentum = 0.08999999612569809\n",
            "loss = 0.02290838398039341 , l_rate = 0.30000000000000004, momentum = 0.009999999776482582\n",
            "loss = 0.022104596719145775 , l_rate = 0.30000000000000004, momentum = 0.019999999552965164\n",
            "loss = 0.019797755405306816 , l_rate = 0.30000000000000004, momentum = 0.029999999329447746\n",
            "loss = 0.02154240757226944 , l_rate = 0.30000000000000004, momentum = 0.03999999910593033\n",
            "loss = 0.020354364067316055 , l_rate = 0.30000000000000004, momentum = 0.04999999701976776\n",
            "loss = 0.022436877712607384 , l_rate = 0.30000000000000004, momentum = 0.059999994933605194\n",
            "loss = 0.020715102553367615 , l_rate = 0.30000000000000004, momentum = 0.07000000029802322\n",
            "loss = 0.02169983647763729 , l_rate = 0.30000000000000004, momentum = 0.07999999821186066\n",
            "loss = 0.019471025094389915 , l_rate = 0.30000000000000004, momentum = 0.08999999612569809\n",
            "loss = 0.02355029061436653 , l_rate = 0.4, momentum = 0.009999999776482582\n",
            "loss = 0.018575109541416168 , l_rate = 0.4, momentum = 0.019999999552965164\n",
            "loss = 0.019890593364834785 , l_rate = 0.4, momentum = 0.029999999329447746\n",
            "loss = 0.01782369799911976 , l_rate = 0.4, momentum = 0.03999999910593033\n",
            "loss = 0.01960238628089428 , l_rate = 0.4, momentum = 0.04999999701976776\n",
            "loss = 0.018461210653185844 , l_rate = 0.4, momentum = 0.059999994933605194\n",
            "loss = 0.021252326667308807 , l_rate = 0.4, momentum = 0.07000000029802322\n",
            "loss = 0.01989700086414814 , l_rate = 0.4, momentum = 0.07999999821186066\n",
            "loss = 0.020919466391205788 , l_rate = 0.4, momentum = 0.08999999612569809\n",
            "loss = 0.018413690850138664 , l_rate = 0.5, momentum = 0.009999999776482582\n",
            "loss = 0.02085116133093834 , l_rate = 0.5, momentum = 0.019999999552965164\n",
            "loss = 0.018625706434249878 , l_rate = 0.5, momentum = 0.029999999329447746\n",
            "loss = 0.018593665212392807 , l_rate = 0.5, momentum = 0.03999999910593033\n",
            "loss = 0.018322207033634186 , l_rate = 0.5, momentum = 0.04999999701976776\n",
            "loss = 0.019329119473695755 , l_rate = 0.5, momentum = 0.059999994933605194\n",
            "loss = 0.018461013212800026 , l_rate = 0.5, momentum = 0.07000000029802322\n",
            "loss = 0.019712533801794052 , l_rate = 0.5, momentum = 0.07999999821186066\n",
            "loss = 0.018890470266342163 , l_rate = 0.5, momentum = 0.08999999612569809\n",
            "loss = 0.01928887702524662 , l_rate = 0.6, momentum = 0.009999999776482582\n",
            "loss = 0.020060135051608086 , l_rate = 0.6, momentum = 0.019999999552965164\n",
            "loss = 0.020900515839457512 , l_rate = 0.6, momentum = 0.029999999329447746\n",
            "loss = 0.019864829257130623 , l_rate = 0.6, momentum = 0.03999999910593033\n",
            "loss = 0.01825784519314766 , l_rate = 0.6, momentum = 0.04999999701976776\n",
            "loss = 0.019075972959399223 , l_rate = 0.6, momentum = 0.059999994933605194\n",
            "loss = 0.019440950825810432 , l_rate = 0.6, momentum = 0.07000000029802322\n",
            "loss = 0.019328827038407326 , l_rate = 0.6, momentum = 0.07999999821186066\n",
            "loss = 0.019410552456974983 , l_rate = 0.6, momentum = 0.08999999612569809\n",
            "loss = 0.020352832973003387 , l_rate = 0.7000000000000001, momentum = 0.009999999776482582\n",
            "loss = 0.02037779986858368 , l_rate = 0.7000000000000001, momentum = 0.019999999552965164\n",
            "loss = 0.020027562975883484 , l_rate = 0.7000000000000001, momentum = 0.029999999329447746\n",
            "loss = 0.01862843707203865 , l_rate = 0.7000000000000001, momentum = 0.03999999910593033\n",
            "loss = 0.02012692764401436 , l_rate = 0.7000000000000001, momentum = 0.04999999701976776\n",
            "loss = 0.01923459954559803 , l_rate = 0.7000000000000001, momentum = 0.059999994933605194\n",
            "loss = 0.018936138600111008 , l_rate = 0.7000000000000001, momentum = 0.07000000029802322\n",
            "loss = 0.019443675875663757 , l_rate = 0.7000000000000001, momentum = 0.07999999821186066\n",
            "loss = 0.02159298025071621 , l_rate = 0.7000000000000001, momentum = 0.08999999612569809\n",
            "loss = 0.020233135670423508 , l_rate = 0.8, momentum = 0.009999999776482582\n",
            "loss = 0.02036123163998127 , l_rate = 0.8, momentum = 0.019999999552965164\n",
            "loss = 0.02103685960173607 , l_rate = 0.8, momentum = 0.029999999329447746\n",
            "loss = 0.01995435357093811 , l_rate = 0.8, momentum = 0.03999999910593033\n",
            "loss = 0.020581740885972977 , l_rate = 0.8, momentum = 0.04999999701976776\n",
            "loss = 0.019533321261405945 , l_rate = 0.8, momentum = 0.059999994933605194\n",
            "loss = 0.023296697065234184 , l_rate = 0.8, momentum = 0.07000000029802322\n",
            "loss = 0.024478277191519737 , l_rate = 0.8, momentum = 0.07999999821186066\n",
            "loss = 0.022079501301050186 , l_rate = 0.8, momentum = 0.08999999612569809\n",
            "loss = 0.022965451702475548 , l_rate = 0.9, momentum = 0.009999999776482582\n",
            "loss = 0.02192981354892254 , l_rate = 0.9, momentum = 0.019999999552965164\n",
            "loss = 0.022283772006630898 , l_rate = 0.9, momentum = 0.029999999329447746\n",
            "loss = 0.021870486438274384 , l_rate = 0.9, momentum = 0.03999999910593033\n",
            "loss = 0.023872191086411476 , l_rate = 0.9, momentum = 0.04999999701976776\n",
            "loss = 0.021655697375535965 , l_rate = 0.9, momentum = 0.059999994933605194\n",
            "loss = 0.02078545093536377 , l_rate = 0.9, momentum = 0.07000000029802322\n",
            "loss = 0.023303376510739326 , l_rate = 0.9, momentum = 0.07999999821186066\n",
            "loss = 0.02201114594936371 , l_rate = 0.9, momentum = 0.08999999612569809\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "final_loss = 0.01782369799911976 , l_rate = 0.4 , glbl_momentum = 0.03999999910593033\n"
          ]
        }
      ],
      "source": [
        "l_rates = np.linspace(0.1,0.9,9)\n",
        "momentas = np.arange(0.01,0.1,0.01,dtype='float32')\n",
        "\n",
        "mn_loss = 1e12\n",
        "glbl_l_rate = None\n",
        "glbl_momentum = None\n",
        "for l_rate in l_rates:\n",
        "  for momentum in momentas:\n",
        "    # Setting Initial Weights For Every Run\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "#     model.layers[1].set_weights(wt_layer1)\n",
        "#     model.layers[2].set_weights(wt_layer2)\n",
        "    loss_ , _ , _ = fmin_loss(-5.0, -4.0 , l_rate, momentum )\n",
        "    print(f'loss = {loss_} , l_rate = {l_rate}, momentum = {momentum}')\n",
        "    if loss_ < mn_loss:\n",
        "      glbl_l_rate = l_rate\n",
        "      glbl_momentum = momentum\n",
        "      mn_loss = loss_\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'final_loss = {mn_loss} , l_rate = {glbl_l_rate} , glbl_momentum = {glbl_momentum}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arwRLnxJpaot"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyvSKtZ-palY"
      },
      "outputs": [],
      "source": [
        "lamdas = np.linspace(-10,0,30, dtype='float32')\n",
        "lamda_grid = [[i,j] for i in lamdas for j in lamdas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNhDh6Mpai7",
        "outputId": "17d61cbd-022c-4251-e5a3-c71c39507344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min  loss at lamda = [-10.0, -10.0] is train_loss = 0.0023097896482795477, val_loss = 0.4026065766811371 , test_loss = 0.34856900572776794\n",
            "min  loss at lamda = [-10.0, -6.8965516] is train_loss = 0.0023713144473731518, val_loss = 0.3958318531513214 , test_loss = 0.3441854417324066\n",
            "min  loss at lamda = [-10.0, -4.137931] is train_loss = 0.00296982005238533, val_loss = 0.39061978459358215 , test_loss = 0.3302921652793884\n",
            "min  loss at lamda = [-10.0, -3.7931035] is train_loss = 0.0032979710958898067, val_loss = 0.37839946150779724 , test_loss = 0.3237793743610382\n",
            "min  loss at lamda = [-10.0, -3.1034484] is train_loss = 0.004341995343565941, val_loss = 0.37025246024131775 , test_loss = 0.31663623452186584\n",
            "min  loss at lamda = [-10.0, -2.7586207] is train_loss = 0.005299258977174759, val_loss = 0.35594385862350464 , test_loss = 0.31400543451309204\n",
            "min  loss at lamda = [-10.0, -2.413793] is train_loss = 0.006753614638000727, val_loss = 0.34627896547317505 , test_loss = 0.2993249297142029\n",
            "min  loss at lamda = [-10.0, -2.0689654] is train_loss = 0.00850756000727415, val_loss = 0.34091833233833313 , test_loss = 0.29329273104667664\n",
            "min  loss at lamda = [-10.0, -1.7241379] is train_loss = 0.010641672648489475, val_loss = 0.33375686407089233 , test_loss = 0.2827070951461792\n",
            "min  loss at lamda = [-10.0, -1.3793104] is train_loss = 0.013019698671996593, val_loss = 0.31997278332710266 , test_loss = 0.27038782835006714\n",
            "min  loss at lamda = [-10.0, -1.0344827] is train_loss = 0.01699935458600521, val_loss = 0.3147546648979187 , test_loss = 0.2692134976387024\n",
            "min  loss at lamda = [-10.0, -0.6896552] is train_loss = 0.023739304393529892, val_loss = 0.3102450966835022 , test_loss = 0.26765191555023193\n",
            "min  loss at lamda = [-10.0, -0.3448276] is train_loss = 0.024744536727666855, val_loss = 0.3098433315753937 , test_loss = 0.26579612493515015\n",
            "min  loss at lamda = [-10.0, 0.0] is train_loss = 0.029268447309732437, val_loss = 0.30698710680007935 , test_loss = 0.26634976267814636\n",
            "min  loss at lamda = [-9.655172, 0.0] is train_loss = 0.03276098892092705, val_loss = 0.30695781111717224 , test_loss = 0.2597779631614685\n",
            "min  loss at lamda = [-8.965517, -0.3448276] is train_loss = 0.02616662159562111, val_loss = 0.30647167563438416 , test_loss = 0.2726559042930603\n",
            "min  loss at lamda = [-8.965517, 0.0] is train_loss = 0.02852689102292061, val_loss = 0.30073046684265137 , test_loss = 0.26139363646507263\n",
            "min  loss at lamda = [-8.620689, 0.0] is train_loss = 0.029708292335271835, val_loss = 0.30064189434051514 , test_loss = 0.266361266374588\n",
            "min  loss at lamda = [-5.5172415, -0.6896552] is train_loss = 0.021658699959516525, val_loss = 0.2977091073989868 , test_loss = 0.26420897245407104\n",
            "min  loss at lamda = [-5.5172415, 0.0] is train_loss = 0.03165239095687866, val_loss = 0.29409998655319214 , test_loss = 0.2619077265262604\n",
            "min  loss at lamda = [-3.1034484, 0.0] is train_loss = 0.03824552521109581, val_loss = 0.29383447766304016 , test_loss = 0.2588706612586975\n",
            "min  loss at lamda = [-2.7586207, -0.6896552] is train_loss = 0.03032897412776947, val_loss = 0.29381489753723145 , test_loss = 0.2629184424877167\n",
            "min  loss at lamda = [-2.7586207, 0.0] is train_loss = 0.041884955018758774, val_loss = 0.29321372509002686 , test_loss = 0.261299192905426\n",
            "min  loss at lamda = [-2.0689654, -0.6896552] is train_loss = 0.03605246543884277, val_loss = 0.29276350140571594 , test_loss = 0.25669801235198975\n",
            "min  loss at lamda = [-2.0689654, -0.3448276] is train_loss = 0.04305574670433998, val_loss = 0.2853154242038727 , test_loss = 0.25271034240722656\n",
            "min  loss at lamda = [-1.3793104, -1.3793104] is train_loss = 0.03315556421875954, val_loss = 0.2813839018344879 , test_loss = 0.24959032237529755\n",
            "min  loss at lamda = [-1.0344827, -0.6896552] is train_loss = 0.05335583537817001, val_loss = 0.28045910596847534 , test_loss = 0.2514621615409851\n",
            "min  loss at lamda = [-0.6896552, -1.0344827] is train_loss = 0.06850776076316833, val_loss = 0.2757387161254883 , test_loss = 0.25563958287239075\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [-0.6896552, -1.0344827] is train_loss = 0.06850776076316833, val_loss = 0.2757387161254883 , test_loss = 0.25563958287239075\n"
          ]
        }
      ],
      "source": [
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in lamda_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  \n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UveWFSMq8o8e"
      },
      "outputs": [],
      "source": [
        "random_lamda = np.random.random(30)     #value will be only between [0,1)\n",
        "ran_lambda = np.sort(10*random_lamda.astype('float32') - 10)\n",
        "ran_grid = [[i,j] for i in ran_lambda for j in ran_lambda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "N90zo3Nc9Lk0",
        "outputId": "8caf1cad-fed9-4e68-bb3c-8fbf359bdff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min  loss at lamda = [-9.789014, -9.789014] is train_loss = 0.002315936842933297, val_loss = 0.40320315957069397 , test_loss = 0.34667351841926575\n",
            "min  loss at lamda = [-9.789014, -8.005441] is train_loss = 0.0023584943264722824, val_loss = 0.40310436487197876 , test_loss = 0.35113292932510376\n",
            "min  loss at lamda = [-9.789014, -5.246722] is train_loss = 0.00250978278927505, val_loss = 0.3965491056442261 , test_loss = 0.33882254362106323\n",
            "min  loss at lamda = [-9.789014, -4.312007] is train_loss = 0.0028718190733343363, val_loss = 0.39222416281700134 , test_loss = 0.3372695744037628\n",
            "min  loss at lamda = [-9.789014, -3.77451] is train_loss = 0.003336251014843583, val_loss = 0.3854410648345947 , test_loss = 0.3276645839214325\n",
            "min  loss at lamda = [-9.789014, -2.809784] is train_loss = 0.005121784284710884, val_loss = 0.37016963958740234 , test_loss = 0.31290870904922485\n",
            "min  loss at lamda = [-9.789014, -2.7225738] is train_loss = 0.005414261016994715, val_loss = 0.3629927337169647 , test_loss = 0.3057555854320526\n",
            "min  loss at lamda = [-9.789014, -2.1182308] is train_loss = 0.008129660040140152, val_loss = 0.3389819860458374 , test_loss = 0.2879185974597931\n",
            "min  loss at lamda = [-9.789014, -1.5014086] is train_loss = 0.013083774596452713, val_loss = 0.32685595750808716 , test_loss = 0.276496559381485\n",
            "min  loss at lamda = [-9.789014, -1.1095104] is train_loss = 0.016979634761810303, val_loss = 0.32320088148117065 , test_loss = 0.2804127633571625\n",
            "min  loss at lamda = [-9.789014, -1.0997772] is train_loss = 0.015906931832432747, val_loss = 0.3077224791049957 , test_loss = 0.2674052119255066\n",
            "min  loss at lamda = [-9.789014, -0.30812168] is train_loss = 0.027849961072206497, val_loss = 0.3021630346775055 , test_loss = 0.26718083024024963\n",
            "min  loss at lamda = [-8.433986, -0.6929474] is train_loss = 0.021336348727345467, val_loss = 0.29905813932418823 , test_loss = 0.26494795083999634\n",
            "min  loss at lamda = [-8.324495, -0.30812168] is train_loss = 0.026323076337575912, val_loss = 0.29256966710090637 , test_loss = 0.2624705731868744\n",
            "min  loss at lamda = [-2.1182308, -1.1095104] is train_loss = 0.027388354763388634, val_loss = 0.2922097444534302 , test_loss = 0.2568390369415283\n",
            "min  loss at lamda = [-1.5014086, -9.3435] is train_loss = 0.012265413999557495, val_loss = 0.2897973358631134 , test_loss = 0.26160183548927307\n",
            "min  loss at lamda = [-1.5014086, -1.0997772] is train_loss = 0.03944885730743408, val_loss = 0.28172215819358826 , test_loss = 0.2620101869106293\n",
            "min  loss at lamda = [-1.1095104, -4.1726933] is train_loss = 0.014346292242407799, val_loss = 0.2803802490234375 , test_loss = 0.2521986663341522\n",
            "min  loss at lamda = [-1.1095104, -2.7225738] is train_loss = 0.023142993450164795, val_loss = 0.27627435326576233 , test_loss = 0.2528074383735657\n",
            "min  loss at lamda = [-0.6929474, -2.1182308] is train_loss = 0.03868251293897629, val_loss = 0.27333083748817444 , test_loss = 0.2553660273551941\n",
            "min  loss at lamda = [-0.30812168, -2.1182308] is train_loss = 0.04374197497963905, val_loss = 0.2687337100505829 , test_loss = 0.2419309765100479\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [-0.30812168, -2.1182308] is train_loss = 0.04374197497963905, val_loss = 0.2687337100505829 , test_loss = 0.2419309765100479\n"
          ]
        }
      ],
      "source": [
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in ran_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  \n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APBn3Tth9V8o"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "old_GS_RS_5K_(60_40)2HP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}