{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7gPDJoxpV7b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er03ye7gpa0h"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXnVjHOPpax8",
        "outputId": "cb5f9875-7219-48dd-d86f-bc93bb3330d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6000, 784), (6000,), (4000, 784), (4000,), (10010, 784), (10010,))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = np.load('train_dataset_10k_60.npz')\n",
        "val_dataset = np.load('validation_dataset_10k_40.npz')\n",
        "test_dataset = np.load('test_dataset.npz')\n",
        "\n",
        "# =========== Loading Datasets ===============\n",
        "\n",
        "x_train = train_dataset['x'].reshape(6000, 784).astype(\"float32\") / 255\n",
        "y_train = train_dataset['y'].astype(\"float32\")\n",
        "  \n",
        "x_val = val_dataset['x'].reshape(4000, 784).astype(\"float32\") / 255\n",
        "y_val = val_dataset['y'].astype(\"float32\")   \n",
        "                    \n",
        "x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n",
        "y_test = test_dataset['y'].astype(\"float32\")                    \n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TgIaOgCpavt"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n",
        "])\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZiEYsk4pate"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits): # Lambda\n",
        "    total_loss = loss_fn(y_train,logits)\n",
        "    return total_loss + (tf.math.exp(lamda1)*tf.nn.l2_loss(w1) + tf.math.exp(lamda2)*tf.nn.l2_loss(w2))/(2*y_train.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_ucMfCjparJ"
      },
      "outputs": [],
      "source": [
        "wt_layer1_init = model.layers[1].get_weights()\n",
        "wt_layer2_init = model.layers[2].get_weights()\n",
        "\n",
        "train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "train_df = train_df.shuffle(buffer_size = 1024).batch(64)\n",
        "\n",
        "def fmin_loss(lamda1, lamda2, l_rate, momentum, epochs= 50, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    tf.keras.backend.clear_session()\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n",
        "    total_loss0 = 1e20\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step,(x_train,y_train) in enumerate(train_df):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_train, training=True)\n",
        "                w1 = model.layers[1].weights[0]\n",
        "                w2 = model.layers[2].weights[0]\n",
        "                total_loss1 = loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits)\n",
        "                \n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "        total_loss0 = total_loss1\n",
        "\n",
        "    wt_layer1 = model.layers[1].get_weights()\n",
        "    wt_layer2 = model.layers[2].get_weights()\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "\n",
        "    return [total_loss1, wt_layer1, wt_layer2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcpmk75yDD5T",
        "outputId": "248e8125-03af-4063-cff9-d149e573ba20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss = 0.03374863788485527 , l_rate = 0.1, momentum = 0.009999999776482582\n",
            "loss = 0.03230967000126839 , l_rate = 0.1, momentum = 0.019999999552965164\n",
            "loss = 0.0493498221039772 , l_rate = 0.1, momentum = 0.029999999329447746\n",
            "loss = 0.0330284982919693 , l_rate = 0.1, momentum = 0.03999999910593033\n",
            "loss = 0.03666314110159874 , l_rate = 0.1, momentum = 0.04999999701976776\n",
            "loss = 0.03719747066497803 , l_rate = 0.1, momentum = 0.059999994933605194\n",
            "loss = 0.03735111281275749 , l_rate = 0.1, momentum = 0.07000000029802322\n",
            "loss = 0.03319472447037697 , l_rate = 0.1, momentum = 0.07999999821186066\n",
            "loss = 0.03180369362235069 , l_rate = 0.1, momentum = 0.08999999612569809\n",
            "loss = 0.02997908741235733 , l_rate = 0.2, momentum = 0.009999999776482582\n",
            "loss = 0.0312800258398056 , l_rate = 0.2, momentum = 0.019999999552965164\n",
            "loss = 0.025714974850416183 , l_rate = 0.2, momentum = 0.029999999329447746\n",
            "loss = 0.02788432687520981 , l_rate = 0.2, momentum = 0.03999999910593033\n",
            "loss = 0.033578187227249146 , l_rate = 0.2, momentum = 0.04999999701976776\n",
            "loss = 0.028502581641077995 , l_rate = 0.2, momentum = 0.059999994933605194\n",
            "loss = 0.023232318460941315 , l_rate = 0.2, momentum = 0.07000000029802322\n",
            "loss = 0.027906358242034912 , l_rate = 0.2, momentum = 0.07999999821186066\n",
            "loss = 0.02647537551820278 , l_rate = 0.2, momentum = 0.08999999612569809\n",
            "loss = 0.027490515261888504 , l_rate = 0.30000000000000004, momentum = 0.009999999776482582\n",
            "loss = 0.02470935322344303 , l_rate = 0.30000000000000004, momentum = 0.019999999552965164\n",
            "loss = 0.025449393317103386 , l_rate = 0.30000000000000004, momentum = 0.029999999329447746\n",
            "loss = 0.02899223566055298 , l_rate = 0.30000000000000004, momentum = 0.03999999910593033\n",
            "loss = 0.023957200348377228 , l_rate = 0.30000000000000004, momentum = 0.04999999701976776\n",
            "loss = 0.028347453102469444 , l_rate = 0.30000000000000004, momentum = 0.059999994933605194\n",
            "loss = 0.025333983823657036 , l_rate = 0.30000000000000004, momentum = 0.07000000029802322\n",
            "loss = 0.026310360059142113 , l_rate = 0.30000000000000004, momentum = 0.07999999821186066\n",
            "loss = 0.024546518921852112 , l_rate = 0.30000000000000004, momentum = 0.08999999612569809\n",
            "loss = 0.026400495320558548 , l_rate = 0.4, momentum = 0.009999999776482582\n",
            "loss = 0.02634248696267605 , l_rate = 0.4, momentum = 0.019999999552965164\n",
            "loss = 0.026458799839019775 , l_rate = 0.4, momentum = 0.029999999329447746\n",
            "loss = 0.02605169266462326 , l_rate = 0.4, momentum = 0.03999999910593033\n",
            "loss = 0.024376418441534042 , l_rate = 0.4, momentum = 0.04999999701976776\n",
            "loss = 0.02539541944861412 , l_rate = 0.4, momentum = 0.059999994933605194\n",
            "loss = 0.026052255183458328 , l_rate = 0.4, momentum = 0.07000000029802322\n",
            "loss = 0.02531430684030056 , l_rate = 0.4, momentum = 0.07999999821186066\n",
            "loss = 0.024879904463887215 , l_rate = 0.4, momentum = 0.08999999612569809\n",
            "loss = 0.025520814582705498 , l_rate = 0.5, momentum = 0.009999999776482582\n",
            "loss = 0.025101616978645325 , l_rate = 0.5, momentum = 0.019999999552965164\n",
            "loss = 0.026329483836889267 , l_rate = 0.5, momentum = 0.029999999329447746\n",
            "loss = 0.024727746844291687 , l_rate = 0.5, momentum = 0.03999999910593033\n",
            "loss = 0.025478914380073547 , l_rate = 0.5, momentum = 0.04999999701976776\n",
            "loss = 0.024681035429239273 , l_rate = 0.5, momentum = 0.059999994933605194\n",
            "loss = 0.024121593683958054 , l_rate = 0.5, momentum = 0.07000000029802322\n",
            "loss = 0.024046167731285095 , l_rate = 0.5, momentum = 0.07999999821186066\n",
            "loss = 0.025488797575235367 , l_rate = 0.5, momentum = 0.08999999612569809\n",
            "loss = 0.02569042518734932 , l_rate = 0.6, momentum = 0.009999999776482582\n",
            "loss = 0.025645123794674873 , l_rate = 0.6, momentum = 0.019999999552965164\n",
            "loss = 0.024837061762809753 , l_rate = 0.6, momentum = 0.029999999329447746\n",
            "loss = 0.02657538466155529 , l_rate = 0.6, momentum = 0.03999999910593033\n",
            "loss = 0.024804674088954926 , l_rate = 0.6, momentum = 0.04999999701976776\n",
            "loss = 0.024428462609648705 , l_rate = 0.6, momentum = 0.059999994933605194\n",
            "loss = 0.027041278779506683 , l_rate = 0.6, momentum = 0.07000000029802322\n",
            "loss = 0.027563337236642838 , l_rate = 0.6, momentum = 0.07999999821186066\n",
            "loss = 0.023798828944563866 , l_rate = 0.6, momentum = 0.08999999612569809\n",
            "loss = 0.025258660316467285 , l_rate = 0.7000000000000001, momentum = 0.009999999776482582\n",
            "loss = 0.02642594464123249 , l_rate = 0.7000000000000001, momentum = 0.019999999552965164\n",
            "loss = 0.02413279004395008 , l_rate = 0.7000000000000001, momentum = 0.029999999329447746\n",
            "loss = 0.02663414552807808 , l_rate = 0.7000000000000001, momentum = 0.03999999910593033\n",
            "loss = 0.026106705889105797 , l_rate = 0.7000000000000001, momentum = 0.04999999701976776\n",
            "loss = 0.024409960955381393 , l_rate = 0.7000000000000001, momentum = 0.059999994933605194\n",
            "loss = 0.023992089554667473 , l_rate = 0.7000000000000001, momentum = 0.07000000029802322\n",
            "loss = 0.025225067511200905 , l_rate = 0.7000000000000001, momentum = 0.07999999821186066\n",
            "loss = 0.0281395111232996 , l_rate = 0.7000000000000001, momentum = 0.08999999612569809\n",
            "loss = 0.026330232620239258 , l_rate = 0.8, momentum = 0.009999999776482582\n",
            "loss = 0.027993056923151016 , l_rate = 0.8, momentum = 0.019999999552965164\n",
            "loss = 0.028593432158231735 , l_rate = 0.8, momentum = 0.029999999329447746\n",
            "loss = 0.02605966106057167 , l_rate = 0.8, momentum = 0.03999999910593033\n",
            "loss = 0.025157324969768524 , l_rate = 0.8, momentum = 0.04999999701976776\n",
            "loss = 0.028222594410181046 , l_rate = 0.8, momentum = 0.059999994933605194\n",
            "loss = 0.024990690872073174 , l_rate = 0.8, momentum = 0.07000000029802322\n",
            "loss = 0.025300031527876854 , l_rate = 0.8, momentum = 0.07999999821186066\n",
            "loss = 0.026305420324206352 , l_rate = 0.8, momentum = 0.08999999612569809\n",
            "loss = 0.027419080957770348 , l_rate = 0.9, momentum = 0.009999999776482582\n",
            "loss = 0.02915576659142971 , l_rate = 0.9, momentum = 0.019999999552965164\n",
            "loss = 0.02540631592273712 , l_rate = 0.9, momentum = 0.029999999329447746\n",
            "loss = 0.027512820437550545 , l_rate = 0.9, momentum = 0.03999999910593033\n",
            "loss = 0.02812437154352665 , l_rate = 0.9, momentum = 0.04999999701976776\n",
            "loss = 0.025538884103298187 , l_rate = 0.9, momentum = 0.059999994933605194\n",
            "loss = 0.02718103490769863 , l_rate = 0.9, momentum = 0.07000000029802322\n",
            "loss = 0.02975326217710972 , l_rate = 0.9, momentum = 0.07999999821186066\n",
            "loss = 0.02810131385922432 , l_rate = 0.9, momentum = 0.08999999612569809\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "final_loss = 0.023232318460941315 , l_rate = 0.2 , glbl_momentum = 0.07000000029802322\n"
          ]
        }
      ],
      "source": [
        "l_rates = np.linspace(0.1,0.9,9)\n",
        "momentas = np.arange(0.01,0.1,0.01,dtype='float32')\n",
        "\n",
        "mn_loss = 1e12\n",
        "glbl_l_rate = None\n",
        "glbl_momentum = None\n",
        "for l_rate in l_rates:\n",
        "  for momentum in momentas:\n",
        "    # Setting Initial Weights For Every Run\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "#     model.layers[1].set_weights(wt_layer1)\n",
        "#     model.layers[2].set_weights(wt_layer2)\n",
        "    loss_ , _ , _ = fmin_loss(-5.0, -4.0 , l_rate, momentum )\n",
        "    print(f'loss = {loss_} , l_rate = {l_rate}, momentum = {momentum}')\n",
        "    if loss_ < mn_loss:\n",
        "      glbl_l_rate = l_rate\n",
        "      glbl_momentum = momentum\n",
        "      mn_loss = loss_\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'final_loss = {mn_loss} , l_rate = {glbl_l_rate} , glbl_momentum = {glbl_momentum}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arwRLnxJpaot"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyvSKtZ-palY"
      },
      "outputs": [],
      "source": [
        "lamdas = np.linspace(-10,0,30, dtype='float32')\n",
        "lamda_grid = [[i,j] for i in lamdas for j in lamdas]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNhDh6Mpai7",
        "outputId": "d22a1064-b7e6-4bd6-9167-1d577f418a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min  loss at lamda = [-10.0, -10.0] is train_loss = 0.004425769671797752, val_loss = 0.1872900128364563 , test_loss = 0.25222790241241455\n",
            "min  loss at lamda = [-10.0, -9.310345] is train_loss = 0.004428269807249308, val_loss = 0.18608330190181732 , test_loss = 0.24929757416248322\n",
            "min  loss at lamda = [-10.0, -8.275862] is train_loss = 0.0044098771177232265, val_loss = 0.1858312040567398 , test_loss = 0.2473851591348648\n",
            "min  loss at lamda = [-10.0, -7.2413793] is train_loss = 0.004395514726638794, val_loss = 0.1837134063243866 , test_loss = 0.24948015809059143\n",
            "min  loss at lamda = [-10.0, -4.827586] is train_loss = 0.004941008519381285, val_loss = 0.18226398527622223 , test_loss = 0.24274249374866486\n",
            "min  loss at lamda = [-10.0, -3.7931035] is train_loss = 0.005830907262861729, val_loss = 0.17781183123588562 , test_loss = 0.23657570779323578\n",
            "min  loss at lamda = [-10.0, -3.4482758] is train_loss = 0.0064658476039767265, val_loss = 0.17337410151958466 , test_loss = 0.23160186409950256\n",
            "min  loss at lamda = [-10.0, -2.7586207] is train_loss = 0.008714762516319752, val_loss = 0.17181803286075592 , test_loss = 0.22514770925045013\n",
            "min  loss at lamda = [-10.0, -2.413793] is train_loss = 0.01088279765099287, val_loss = 0.16793836653232574 , test_loss = 0.21847398579120636\n",
            "min  loss at lamda = [-10.0, -2.0689654] is train_loss = 0.013711338862776756, val_loss = 0.1635531336069107 , test_loss = 0.21222683787345886\n",
            "min  loss at lamda = [-10.0, -1.7241379] is train_loss = 0.016918787732720375, val_loss = 0.1584218591451645 , test_loss = 0.20636366307735443\n",
            "min  loss at lamda = [-10.0, -1.0344827] is train_loss = 0.02824140340089798, val_loss = 0.15635143220424652 , test_loss = 0.19938135147094727\n",
            "min  loss at lamda = [-7.2413793, -1.0344827] is train_loss = 0.02597231976687908, val_loss = 0.15572205185890198 , test_loss = 0.199667289853096\n",
            "min  loss at lamda = [-5.172414, -1.0344827] is train_loss = 0.02771955356001854, val_loss = 0.15539932250976562 , test_loss = 0.19866743683815002\n",
            "min  loss at lamda = [-3.7931035, -1.3793104] is train_loss = 0.02490416169166565, val_loss = 0.1544228345155716 , test_loss = 0.19934815168380737\n",
            "min  loss at lamda = [-2.7586207, -2.0689654] is train_loss = 0.019669760018587112, val_loss = 0.15336336195468903 , test_loss = 0.1976231038570404\n",
            "min  loss at lamda = [-1.7241379, -6.8965516] is train_loss = 0.01571887917816639, val_loss = 0.15286508202552795 , test_loss = 0.2017495483160019\n",
            "min  loss at lamda = [-1.7241379, -3.7931035] is train_loss = 0.017765600234270096, val_loss = 0.15155428647994995 , test_loss = 0.1982503980398178\n",
            "min  loss at lamda = [-1.3793104, -8.620689] is train_loss = 0.018116381019353867, val_loss = 0.14870809018611908 , test_loss = 0.19255271553993225\n",
            "min  loss at lamda = [-0.6896552, -6.8965516] is train_loss = 0.030055813491344452, val_loss = 0.146209716796875 , test_loss = 0.19095909595489502\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [-0.6896552, -6.8965516] is train_loss = 0.030055813491344452, val_loss = 0.146209716796875 , test_loss = 0.19095909595489502\n"
          ]
        }
      ],
      "source": [
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in lamda_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  \n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UveWFSMq8o8e"
      },
      "outputs": [],
      "source": [
        "random_lamda = np.random.random(30)     #value will be only between [0,1)\n",
        "ran_lambda = np.sort(10*random_lamda.astype('float32') - 10)\n",
        "ran_grid = [[i,j] for i in ran_lambda for j in ran_lambda]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N90zo3Nc9Lk0",
        "outputId": "1122e769-4b8a-4b01-c5f4-2138b24b8ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "min  loss at lamda = [-9.645136, -9.645136] is train_loss = 0.004438948817551136, val_loss = 0.18946228921413422 , test_loss = 0.250110924243927\n",
            "min  loss at lamda = [-9.645136, -8.403118] is train_loss = 0.0043986160308122635, val_loss = 0.18637166917324066 , test_loss = 0.24713745713233948\n",
            "min  loss at lamda = [-9.645136, -6.304161] is train_loss = 0.004547660704702139, val_loss = 0.1848156452178955 , test_loss = 0.2505200505256653\n",
            "min  loss at lamda = [-9.645136, -5.3244348] is train_loss = 0.004686613101512194, val_loss = 0.18370741605758667 , test_loss = 0.24619096517562866\n",
            "min  loss at lamda = [-9.645136, -4.009692] is train_loss = 0.005494807381182909, val_loss = 0.18079005181789398 , test_loss = 0.24162936210632324\n",
            "min  loss at lamda = [-9.645136, -3.8369074] is train_loss = 0.005848594009876251, val_loss = 0.1783258467912674 , test_loss = 0.2404467910528183\n",
            "min  loss at lamda = [-9.645136, -3.5579185] is train_loss = 0.006328620482236147, val_loss = 0.17770907282829285 , test_loss = 0.23591284453868866\n",
            "min  loss at lamda = [-9.645136, -3.2364569] is train_loss = 0.007172501180320978, val_loss = 0.1774022877216339 , test_loss = 0.2312120795249939\n",
            "min  loss at lamda = [-9.645136, -3.1869073] is train_loss = 0.0071215545758605, val_loss = 0.1765289008617401 , test_loss = 0.231587216258049\n",
            "min  loss at lamda = [-9.645136, -2.986341] is train_loss = 0.0076805646531283855, val_loss = 0.17289797961711884 , test_loss = 0.22664469480514526\n",
            "min  loss at lamda = [-9.645136, -2.934063] is train_loss = 0.008121935650706291, val_loss = 0.17199349403381348 , test_loss = 0.22343142330646515\n",
            "min  loss at lamda = [-9.645136, -2.905446] is train_loss = 0.008139663375914097, val_loss = 0.17178502678871155 , test_loss = 0.22617778182029724\n",
            "min  loss at lamda = [-9.645136, -2.4833918] is train_loss = 0.010405810549855232, val_loss = 0.16876770555973053 , test_loss = 0.2197396159172058\n",
            "min  loss at lamda = [-9.645136, -2.385501] is train_loss = 0.01098394300788641, val_loss = 0.16576425731182098 , test_loss = 0.21523471176624298\n",
            "min  loss at lamda = [-9.645136, -1.7363749] is train_loss = 0.017175570130348206, val_loss = 0.1647377908229828 , test_loss = 0.21096335351467133\n",
            "min  loss at lamda = [-9.645136, -1.5020657] is train_loss = 0.01963479444384575, val_loss = 0.16213785111904144 , test_loss = 0.21041923761367798\n",
            "min  loss at lamda = [-9.645136, -1.0858192] is train_loss = 0.02652975171804428, val_loss = 0.15837731957435608 , test_loss = 0.20241712033748627\n",
            "min  loss at lamda = [-9.34908, -0.5479584] is train_loss = 0.03635522350668907, val_loss = 0.157930389046669 , test_loss = 0.19602763652801514\n",
            "min  loss at lamda = [-8.507517, -1.0858192] is train_loss = 0.025391127914190292, val_loss = 0.15660275518894196 , test_loss = 0.1981579214334488\n",
            "min  loss at lamda = [-6.596493, -1.5020657] is train_loss = 0.020358802750706673, val_loss = 0.15648436546325684 , test_loss = 0.20256371796131134\n",
            "min  loss at lamda = [-3.8369074, -1.7363749] is train_loss = 0.019069913774728775, val_loss = 0.1558302491903305 , test_loss = 0.20128390192985535\n",
            "min  loss at lamda = [-3.1869073, -1.7363749] is train_loss = 0.02165188640356064, val_loss = 0.1549757421016693 , test_loss = 0.1999601572751999\n",
            "min  loss at lamda = [-2.934063, -1.5020657] is train_loss = 0.026155143976211548, val_loss = 0.15496961772441864 , test_loss = 0.19736114144325256\n",
            "min  loss at lamda = [-2.4833918, -2.905446] is train_loss = 0.015266947448253632, val_loss = 0.15414148569107056 , test_loss = 0.20285390317440033\n",
            "min  loss at lamda = [-2.4833918, -2.4833918] is train_loss = 0.019153103232383728, val_loss = 0.15412428975105286 , test_loss = 0.20311319828033447\n",
            "min  loss at lamda = [-2.4833918, -1.7363749] is train_loss = 0.026166336610913277, val_loss = 0.15398336946964264 , test_loss = 0.19603212177753448\n",
            "min  loss at lamda = [-2.385501, -2.934063] is train_loss = 0.015539410524070263, val_loss = 0.15221655368804932 , test_loss = 0.2004133015871048\n",
            "min  loss at lamda = [-1.7363749, -3.1869073] is train_loss = 0.020590927451848984, val_loss = 0.14999425411224365 , test_loss = 0.1959112286567688\n",
            "min  loss at lamda = [-1.0858192, -5.3244348] is train_loss = 0.024773389101028442, val_loss = 0.14893902838230133 , test_loss = 0.1940988451242447\n",
            "min  loss at lamda = [-0.5479584, -4.009692] is train_loss = 0.03221489489078522, val_loss = 0.14736469089984894 , test_loss = 0.18652239441871643\n",
            "min  loss at lamda = [-0.23704433, -5.08272] is train_loss = 0.03280287981033325, val_loss = 0.14380408823490143 , test_loss = 0.1830233484506607\n",
            "###############################################################\n",
            "--------------------Result-------------------------------------\n",
            "min  loss at lamda = [-0.23704433, -5.08272] is train_loss = 0.03280287981033325, val_loss = 0.14380408823490143 , test_loss = 0.1830233484506607\n"
          ]
        }
      ],
      "source": [
        "loss_from_val = 1000000000\n",
        "loss_from_train = None\n",
        "loss_from_test = None\n",
        "corres_lamda = None\n",
        "\n",
        "for lamda in ran_grid:\n",
        "  model.layers[1].set_weights(wt_layer1_init)\n",
        "  model.layers[2].set_weights(wt_layer2_init)\n",
        "  \n",
        "  min_loss,final_wt1,final_wt2 = fmin_loss(lamda[0],lamda[1],glbl_l_rate,glbl_momentum)\n",
        "  \n",
        "  model.layers[1].set_weights(final_wt1)\n",
        "  model.layers[2].set_weights(final_wt2)\n",
        "  \n",
        "  val_logits = model(x_val,training=False)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "  train_logits = model(x_train,training=False)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "  \n",
        "  test_logits = model(x_test,training=False)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "  if val_loss < loss_from_val:\n",
        "    loss_from_val = val_loss\n",
        "    loss_from_train = training_loss\n",
        "    loss_from_test = test_loss\n",
        "    corres_lamda = lamda\n",
        "    print(f'min  loss at lamda = {lamda} is train_loss = {training_loss}, val_loss = {val_loss} , test_loss = {test_loss}')\n",
        "\n",
        "print('###############################################################')\n",
        "print('--------------------Result-------------------------------------')\n",
        "print(f'min  loss at lamda = {corres_lamda} is train_loss = {loss_from_train}, val_loss = {loss_from_val} , test_loss = {loss_from_test}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APBn3Tth9V8o"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "old_GS_RS_10K_(60_40)2HP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}