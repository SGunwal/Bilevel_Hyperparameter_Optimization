{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "ceda8cd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "ceda8cd2"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "hUbsx_7MelZh"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "id": "hUbsx_7MelZh"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "951d09f2",
        "outputId": "d33a4462-1f8b-40b9-e29e-c6d069754938"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((600, 784), (600,), (400, 784), (400,), (10010, 784), (10010,))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "train_dataset = np.load('train_dataset1_60.npz')\n",
        "val_dataset = np.load('validation_dataset1_40.npz')\n",
        "test_dataset = np.load('test_dataset.npz')\n",
        "\n",
        "# =========== Loading Datasets ===============\n",
        "\n",
        "x_train = train_dataset['x'].reshape(600, 784).astype(\"float32\") / 255\n",
        "y_train = train_dataset['y'].astype(\"float32\")\n",
        "  \n",
        "x_val = val_dataset['x'].reshape(400, 784).astype(\"float32\") / 255\n",
        "y_val = val_dataset['y'].astype(\"float32\")   \n",
        "                    \n",
        "x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n",
        "y_test = test_dataset['y'].astype(\"float32\")                    \n",
        "\n",
        "\n",
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "
      ],
      "id": "951d09f2"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "afefa986"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n",
        "])\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "id": "afefa986"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "cef9ecb3"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits): # Lambda\n",
        "    total_loss = loss_fn(y_train,logits)\n",
        "    return total_loss + (tf.math.exp(lamda1)*tf.nn.l2_loss(w1)+tf.math.exp(lamda2)*tf.nn.l2_loss(w2))/(2*y_train.shape[0])"
      ],
      "id": "cef9ecb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba4b12ef"
      },
      "source": [
        "#### $\\phi(\\lambda) = min\\{l(w;S^T) + \\exp{\\lambda}(\\|w\\|_2^2)\\}$ for given $\\lambda$"
      ],
      "id": "ba4b12ef"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "48045099"
      },
      "outputs": [],
      "source": [
        "wt_layer1_init = model.layers[1].get_weights()\n",
        "wt_layer2_init = model.layers[2].get_weights()\n",
        "\n",
        "train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
        "train_df = train_df.shuffle(buffer_size = 1024).batch(64)\n",
        "\n",
        "def fmin_loss(lamda1, lamda2, epochs= 50, l_rate= 0.5, momentum= 0.07999, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n",
        "    tf.keras.backend.clear_session()\n",
        "    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n",
        "    total_loss0 = 1e20\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for step,(x_train,y_train) in enumerate(train_df):\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x_train, training=True)\n",
        "                w1 = model.layers[1].weights[0]\n",
        "                w2 = model.layers[2].weights[0]\n",
        "                total_loss1 = loss(w1,w2,lamda1, lamda2,loss_fn,y_train,logits)\n",
        "                \n",
        "            vars_list = model.trainable_weights\n",
        "            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n",
        "            optimizer.apply_gradients(zip(grads,vars_list))\n",
        "\n",
        "        total_loss0 = total_loss1\n",
        "\n",
        "    wt_layer1 = model.layers[1].get_weights()\n",
        "    wt_layer2 = model.layers[2].get_weights()\n",
        "    model.layers[1].set_weights(wt_layer1_init)\n",
        "    model.layers[2].set_weights(wt_layer2_init)\n",
        "\n",
        "    return [total_loss1, wt_layer1, wt_layer2]"
      ],
      "id": "48045099"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "c176e652"
      },
      "outputs": [],
      "source": [
        "def f_val(w1,b1,w2,b2,lamda1, lamda2, xtr, ytr): # lamda -> Variable\n",
        "    layer1_weights = [w1,b1]\n",
        "    layer2_weights = [w2,b2]\n",
        "    model.layers[1].set_weights(layer1_weights)\n",
        "    model.layers[2].set_weights(layer2_weights)\n",
        "    logits = model(xtr)\n",
        "    return loss(w1,w2,lamda1, lamda2,loss_fn,ytr,logits)\n",
        "\n",
        "def f_grad(w1,b1,w2,b2,lamda1, lamda2, xtr, ytr): # lamda -> Variable\n",
        "    with tf.GradientTape() as tape:\n",
        "        total_loss = f_val(w1,b1,w2,b2,lamda1, lamda2, xtr, ytr)\n",
        "    vars_list = model.trainable_weights\n",
        "    vars_list.append(lamda1)\n",
        "    vars_list.append(lamda2)\n",
        "    grads = tape.gradient(total_loss, vars_list) \n",
        "    return grads\n",
        "\n",
        "def F_val(w1,b1,w2,b2, xval, yval):     \n",
        "    layer1_weights = [w1,b1]\n",
        "    layer2_weights = [w2,b2]\n",
        "    model.layers[1].set_weights(layer1_weights)\n",
        "    model.layers[2].set_weights(layer2_weights)\n",
        "    logits = model(xval)\n",
        "    return loss_fn(yval,logits)\n",
        "\n",
        "def F_grad(w1,b1,w2,b2, xval, yval):   # float32 arrays\n",
        "    with tf.GradientTape() as tape:\n",
        "        total_loss = F_val(w1,b1,w2,b2,xval,yval)\n",
        "    vars_list = model.trainable_weights\n",
        "    grads = tape.gradient(total_loss, vars_list) \n",
        "    return grads"
      ],
      "id": "c176e652"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e33981fc"
      },
      "source": [
        "### Gaussian Process Regressor"
      ],
      "id": "e33981fc"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "f994b19c"
      },
      "outputs": [],
      "source": [
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def krig_conversion( lamdas ):\n",
        "    Lam = []\n",
        "    for lamda in lamdas:\n",
        "        lamda = lamda.numpy()\n",
        "        if np.isscalar(lamda) == True:\n",
        "            Lam.append( lamda)\n",
        "        else:\n",
        "            Lam.append( lamda[0] )\n",
        "    return tf.Variable(Lam, dtype = 'float32')\n",
        "\n",
        "# NOTE: Function for prediction at multi-dimensional lamda, not multiple lamdas\n",
        "def GPR( Lamda_sample, Phi_sample, lamda1, lamda2 ): # ( arr([[],[],...]), arr([[],[],...]), arr([[],[],...])  )\n",
        "\n",
        "    lamdas = [lamda1, lamda2]\n",
        "    x = np.array([krig_conversion( lamdas)])\n",
        "    kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
        "    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
        "    gaussian_process.fit(Lamda_sample, Phi_sample)\n",
        "\n",
        "    k2_l = gaussian_process.kernel_.get_params()['k2__length_scale']\n",
        "    y_pred, sigma = gaussian_process.predict(x, return_std=True)\n",
        "    y_pred_grad = 0.0*y_pred\n",
        "\n",
        "    k_val = gaussian_process.kernel_( Lamda_sample , np.atleast_2d(x) , eval_gradient=False).ravel() \n",
        "    x_diff_over_l_sq = ([lam_val-x for lam_val in Lamda_sample]/np.power(k2_l,2))#.ravel()\n",
        "    intermediate_result = [k*i for k,i in zip(k_val,x_diff_over_l_sq)]\n",
        "    final_result = 0\n",
        "    for alpha, int_res in zip(gaussian_process.alpha_, intermediate_result):\n",
        "        final_result += alpha*int_res\n",
        "    final_result *= gaussian_process._y_train_std\n",
        "    \n",
        "    return [y_pred, final_result]\n"
      ],
      "id": "f994b19c"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "5394a874"
      },
      "outputs": [],
      "source": [
        "def Function_Grad( w1,b1,w2,b2,lamda1,lamda2, Lamda_sample, Phi_sample, R_val, Mu_val,\n",
        "                  xtr, ytr, xval, yval ):\n",
        "\n",
        "    Phi = GPR( Lamda_sample, Phi_sample,lamda1,lamda2)\n",
        "    Phi_val, Phi_grad = Phi[0], Phi[1]\n",
        "    \n",
        "    p1 = F_grad( w1,b1,w2,b2, xval, yval ) \n",
        "    p2 = ( R_val* ( f_val(w1,b1,w2,b2,lamda1,lamda2,xtr,ytr) -  Phi_val) + Mu_val )\n",
        "\n",
        "    # print(\"\\n Lamda1,2 = \", lamda1, lamda2)\n",
        "\n",
        "    f_gradient = f_grad(w1,b1,w2,b2,lamda1,lamda2,xtr,ytr)\n",
        "\n",
        "    p3 = f_gradient[:-2] + [f_gradient[-2].numpy()-Phi_grad[0][0],f_gradient[-1].numpy()-Phi_grad[0][1]]\n",
        "    p2tp3 = [p2*elt for elt in p3]\n",
        "    gradients = [e1+e2 for e1,e2 in zip(p1,p2tp3[:-2])] + p2tp3[-2:]\n",
        "    gradients = [(tf.clip_by_norm(grad, clip_norm = 2.0)) for grad in gradients]\n",
        "    return gradients   # Output : w1, b1, w2, b2, lamda : floar32 arrays"
      ],
      "id": "5394a874"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "9afbbbe3"
      },
      "outputs": [],
      "source": [
        "def AugLag_Function( w1,b1,w2,b2,lamda1, lamda2, Lamda_sample, Phi_sample, R_val, Mu_val, xtr, ytr, xval, yval ):\n",
        "    PHI = GPR( Lamda_sample, Phi_sample, lamda1,lamda2 )\n",
        "    f_phi = f_val(w1,b1,w2,b2,lamda1,lamda2,xtr,ytr) - PHI[0]\n",
        "    final_val = F_val(w1,b1,w2,b2,xval,yval) + (R_val/2) * f_phi**2 + Mu_val * f_phi\n",
        "    return final_val.numpy()[0]"
      ],
      "id": "9afbbbe3"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "10c95548"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def Gradient_Descent(  w1,b1,w2,b2,lamda1,lamda2,Lamda_sample, Phi_sample, R_val, Mu_val, momentum, learning_rate,\n",
        "                     batch_size=64, n_iter=300, F_tolerance=1e-6, opt_tolerance = 1e-10, dtype=\"float32\", random_state=None ):\n",
        "\n",
        "    w10,b10,w20,b20,lamda10,lamda20 = w1,b1,w2,b2,lamda1,lamda2\n",
        "\n",
        "    seed   = None if random_state is None else int(random_state)\n",
        "    rng    = np.random.default_rng(seed=seed)\n",
        "    \n",
        "    n_obs_tr  = x_train.shape[0]\n",
        "    n_obs_val = x_val.shape[0]\n",
        "    xy_train  = np.c_[x_train.reshape(n_obs_tr, -1), y_train.reshape(n_obs_tr, 1)]\n",
        "    xy_val    = np.c_[x_val.reshape(n_obs_val, -1) , y_val.reshape(n_obs_val, 1) ]\n",
        "\n",
        "    #print(\"\\n Shapes tr,val = \", xy_train.shape, xy_val.shape)\n",
        "\n",
        "    vector     = np.array( [w10,b10,w20,b20,lamda10,lamda20] )\n",
        "\n",
        "    learn_rate = np.array(learning_rate, dtype=dtype)\n",
        "    batch_size = int(batch_size)\n",
        "    n_iter     = int(n_iter)\n",
        "    fun_tolerance  = np.array(F_tolerance, dtype=dtype)\n",
        "    diff_old       = np.array([0 for i in range(0,6)])  # difference_initial\n",
        "\n",
        "    Z_val0, grad_old, loss_ = 1e20, 1e10, 1e20\n",
        "\n",
        "    Z_val_track = []\n",
        "\n",
        "    for _ in range(n_iter):\n",
        "        \n",
        "        rng.shuffle(xy_train)\n",
        "        rng.shuffle(xy_val)\n",
        "\n",
        "        idx_tr = np.random.randint( n_obs_tr, size=batch_size )\n",
        "        idx_val = np.random.randint( n_obs_val, size=batch_size )\n",
        "        #print(idx_tr,\"\\n\\n\",idx_val)\n",
        "        x_batch_tr, y_batch_tr = xy_train[idx_tr, :-1], xy_train[idx_tr, -1:]\n",
        "        x_batch_val, y_batch_val = xy_val[idx_val, :-1], xy_val[idx_val, -1:]\n",
        "\n",
        "        Z_grad = Function_Grad( w10,b10,w20,b20,lamda10,lamda20, Lamda_sample, Phi_sample, R_val, Mu_val, x_batch_tr, y_batch_tr, x_batch_val, y_batch_val )\n",
        "        Z_grad = np.array(Z_grad)\n",
        "        diff_new = learning_rate*Z_grad\n",
        "\n",
        "        [w10,b10,w20,b20,lamda10,lamda20] = np.array([w10,b10,w20,b20,lamda10,lamda20]) - ( learning_rate*diff_new + momentum*diff_old )\n",
        "        diff_old = diff_new\n",
        "        lamda10 = tf.Variable(lamda10, dtype = dtype)\n",
        "        lamda20 = tf.Variable(lamda20, dtype = dtype)\n",
        "\n",
        "        # ============= Termination Criterias Check ========================\n",
        "\n",
        "        Z_val = AugLag_Function( w10,b10,w20,b20,lamda10,lamda20, Lamda_sample, Phi_sample, R_val, Mu_val, x_train, y_train, x_val, y_val )\n",
        "        \n",
        "        grad_tol = np.array( [np.linalg.norm(g, ord = np.inf) for g in Z_grad ] )\n",
        "\n",
        "        if Z_val <= loss_ :\n",
        "            \n",
        "            opt_weights = [w10,b10,w20,b20,lamda10,lamda20]\n",
        "            fun_tol = abs( ( Z_val - loss_ )/(1+abs( loss_ )) )\n",
        "            if  fun_tol <= F_tolerance and np.all(grad_tol) <= opt_tolerance:\n",
        "                loss_ = Z_val\n",
        "                break\n",
        "            loss_ = Z_val\n",
        "          \n",
        "           \n",
        "    # plt.plot(np.array(Z_val_track), linestyle = 'dotted')\n",
        "    # plt.show()\n",
        "    return opt_weights , loss_"
      ],
      "id": "10c95548"
    },
    {
      "cell_type": "code",
      "source": [
        "def LOSS_FUNCTION( wts1, wts2 ):\n",
        "  \n",
        "  model.layers[1].set_weights(wts1)\n",
        "  model.layers[2].set_weights(wts2)\n",
        "\n",
        "  val_logits = model(x_val)\n",
        "  val_loss = loss_fn(y_val,val_logits)\n",
        "\n",
        "  train_logits = model(x_train)\n",
        "  training_loss = loss_fn(y_train,train_logits)\n",
        "\n",
        "  test_logits = model(x_test)\n",
        "  test_loss = loss_fn(y_test,test_logits)\n",
        "\n",
        "  return val_loss, training_loss, test_loss"
      ],
      "metadata": {
        "id": "996oHO4xT5I8"
      },
      "id": "996oHO4xT5I8",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1bad8b81"
      },
      "outputs": [],
      "source": [
        "# GLOBAL : Lamda_sample, Phi_sample\n",
        "def Augmented_Lagrangian( w1, b1, w2, b2, lamda1, lamda2, Lamda_sample, Phi_sample, mom, lr, R_val = 2, Mu_val = 2, neta = 1.5, al_epochs = 5 ):\n",
        "    Z_val0 = 1e+20\n",
        "    Lamda_sampleL, Phi_sampleL = Lamda_sample, Phi_sample\n",
        "    wts_opt0, lamda_opt10, lamda_opt20, min_violation = [], 0, 0, 10\n",
        "    for epoch in range(al_epochs):\n",
        "\n",
        "        Opt_Weights, _ = Gradient_Descent( w1,b1,w2,b2,lamda1, lamda2, Lamda_sampleL, Phi_sampleL, R_val, Mu_val, mom, lr )\n",
        "        w1_opt,b1_opt,w2_opt,b2_opt, lamda_opt1, lamda_opt2 = Opt_Weights[0], Opt_Weights[1], Opt_Weights[2], Opt_Weights[3], Opt_Weights[4],Opt_Weights[5]  # lamda not var\n",
        "        lamda_opt1 = tf.Variable(lamda_opt1, dtype=tf.float32)        \n",
        "        lamda_opt2 = tf.Variable(lamda_opt2, dtype=tf.float32)\n",
        "\n",
        "        valL, trL, testL = LOSS_FUNCTION( [w1_opt,b1_opt], [w2_opt,b2_opt] )\n",
        "\n",
        "        # ========== NOT USED ====================\n",
        "        w1,b1,w2,b2,lamda1,lamda2 = w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt1,lamda_opt2 \n",
        "        Phi1 = GPR(Lamda_sampleL, Phi_sampleL, lamda1, lamda2)\n",
        "        Phi_val1 = Phi1[0]\n",
        "\n",
        "        # ==========UPDATING LAMBDA SAMPLE FOR GPR=========================\n",
        "        \n",
        "        Lamda_sampleL = np.array(np.vstack([Lamda_sampleL,[lamda_opt1.numpy()[0],lamda_opt2.numpy()[0]]]), dtype = 'float32')\n",
        "        index = np.where(Lamda_sampleL == [lamda_opt1.numpy()[0],lamda_opt2.numpy()[0]] )\n",
        "        Phi_new_lamda = fmin_loss(lamda_opt1,lamda_opt2)[0].numpy()\n",
        "        Phi_sampleL = np.insert(Phi_sampleL, index[0][0], Phi_new_lamda)\n",
        "\n",
        "        #print(\"\\n Phi_new_lamda = \", Phi_new_lamda)\n",
        "\n",
        "        # ============== Updating Augmented Lagrangian Parameters ====================\n",
        "        constraint_ = f_val(w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt1,lamda_opt2,x_train,y_train) - Phi_val1\n",
        "        Mu_val = Mu_val + R_val*( constraint_ )\n",
        "        R_val  = neta*R_val\n",
        "\n",
        "        # ===== Constraint Violation Criteria ===================\n",
        "        violation = abs(constraint_)\n",
        "        print(\"\\n\\n Violation = \", violation, \" Lambda = \",lamda_opt1.numpy()[0],lamda_opt2.numpy()[0], f\" Train_Loss : {trL}, Val_Loss : {valL}, Test_Loss : {testL}\")\n",
        "        #print(f\"\\n {epoch} :: Violation = {violation}, Lambdas = {lamda_opt1.numpy(), lamda_opt2.numpy()}\")\n",
        "        if violation <= min_violation:\n",
        "          wts_opt0 = [ [w1_opt, b1_opt], [w2_opt, b2_opt]]\n",
        "          lamda_opt10, lamda_opt20 = lamda_opt1, lamda_opt2\n",
        "          min_violation = violation\n",
        "      \n",
        "    #print(f\"\\n\\n AL Ended With Violation :: {min_violation}, Optimal Lambdas = {lamda_opt10, lamda_opt20}\")\n",
        "    return wts_opt0, lamda_opt10, lamda_opt20"
      ],
      "id": "1bad8b81"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cbc32ef",
        "outputId": "0b57f904-d531-49f3-af1f-35cac4582e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Phi_sample:  [6.9671160e-01 3.3661717e-01 4.1238612e-01 6.0323358e-01 7.8554136e-01\n",
            " 3.3065301e-01 1.2318026e-01 7.1269505e-02 5.5065643e-02 4.5411229e-02\n",
            " 1.8493716e-01 3.3277422e-02 1.3053998e-02 1.0394170e-02 9.8602055e-03\n",
            " 1.2711321e-01 1.9395554e-02 4.0507717e-03 2.0850329e-03 1.8189733e-03\n",
            " 1.0122689e-01 1.6354559e-02 2.2305856e-03 5.3336367e-04 6.8115350e-04] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Lamda_sample1 = np.linspace( 0, -10, num = 5, endpoint = True, dtype = 'float32' )\n",
        "\n",
        "Lamda_sample = []\n",
        "for x in Lamda_sample1:\n",
        "    for y in Lamda_sample1:\n",
        "            Lamda_sample.append( [x, y] )\n",
        "Lamda_sample = np.array(Lamda_sample)\n",
        "\n",
        "Phi_sample, Weights, lamda_init = [], [], 0\n",
        "init_val_loss = 1e20\n",
        "\n",
        "for lamda in Lamda_sample:\n",
        "    \n",
        "    [min_loss, layer1wt, layer2wt] = fmin_loss(lamda[0],lamda[1])\n",
        "    Phi_sample.append( min_loss.numpy() )\n",
        "    \n",
        "    model.layers[1].set_weights(layer1wt)\n",
        "    model.layers[2].set_weights(layer2wt)\n",
        "    val_logits = model(x_val)\n",
        "    val_loss = loss_fn(y_val,val_logits)\n",
        "    if val_loss <= init_val_loss :\n",
        "        init_val_loss = val_loss\n",
        "        Weights = [layer1wt, layer2wt]\n",
        "        lamda_init1 = lamda[0]\n",
        "        lamda_init2 = lamda[1]\n",
        "\n",
        "Phi_sample = np.array(Phi_sample)\n",
        "print(\"\\n\\n Phi_sample: \", Phi_sample, \"\\n\\n\")\n",
        "\n",
        "#============================== Initializing Weights =================================================\n",
        "w1, b1 = Weights[0][0], Weights[0][1]\n",
        "w2, b2 = Weights[1][0], Weights[1][1]\n",
        "lamda1 = tf.Variable(lamda_init1, dtype = tf.float32)\n",
        "lamda2 = tf.Variable(lamda_init2, dtype = tf.float32)\n"
      ],
      "id": "5cbc32ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqd0wgrrSn1w"
      },
      "source": [
        "### Tuning GD"
      ],
      "id": "pqd0wgrrSn1w"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtLZ4_yjSfrY",
        "outputId": "d7293e52-fd60-4a50-ad79-9312b3ddbe8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " LR : 0.0010000000474974513, MOM : 0.1, Z_VAL : -0.3622509241104126\n",
            "\n",
            " LR : 0.0010000000474974513, MOM : 0.5, Z_VAL : -0.37301987409591675\n",
            "\n",
            " LR : 0.0010000000474974513, MOM : 0.8, Z_VAL : -0.37914133071899414\n",
            "\n",
            " LR : 0.0010000000474974513, MOM : 0.9, Z_VAL : -0.3810077905654907\n",
            "\n",
            " LR : 0.0010000000474974513, MOM : 0.99, Z_VAL : -0.3834330439567566\n",
            "\n",
            " LR : 0.009999999776482582, MOM : 0.1, Z_VAL : -0.38525497913360596\n",
            "\n",
            " LR : 0.009999999776482582, MOM : 0.5, Z_VAL : -0.4201756715774536\n",
            "\n",
            " LR : 0.009999999776482582, MOM : 0.8, Z_VAL : -0.43406254053115845\n",
            "\n",
            " LR : 0.009999999776482582, MOM : 0.9, Z_VAL : -0.4413493871688843\n",
            "\n",
            " LR : 0.009999999776482582, MOM : 0.99, Z_VAL : -0.44184744358062744\n",
            "\n",
            " LR : 0.05000000074505806, MOM : 0.1, Z_VAL : -0.4320727586746216\n",
            "\n",
            " LR : 0.05000000074505806, MOM : 0.5, Z_VAL : -0.5020930171012878\n",
            "\n",
            " LR : 0.05000000074505806, MOM : 0.8, Z_VAL : -0.5342419147491455\n",
            "\n",
            " LR : 0.05000000074505806, MOM : 0.9, Z_VAL : -0.5425843000411987\n",
            "\n",
            " LR : 0.05000000074505806, MOM : 0.99, Z_VAL : -0.5499176979064941\n",
            "\n",
            " LR : 0.07000000029802322, MOM : 0.1, Z_VAL : -0.45174896717071533\n",
            "\n",
            " LR : 0.07000000029802322, MOM : 0.5, Z_VAL : -0.5300949215888977\n",
            "\n",
            " LR : 0.07000000029802322, MOM : 0.8, Z_VAL : -0.5643596649169922\n",
            "\n",
            " LR : 0.07000000029802322, MOM : 0.9, Z_VAL : -0.5760258436203003\n",
            "\n",
            " LR : 0.07000000029802322, MOM : 0.99, Z_VAL : -0.5868147611618042\n",
            "\n",
            " LR : 0.10000000149011612, MOM : 0.1, Z_VAL : -0.4760066270828247\n",
            "\n",
            " LR : 0.10000000149011612, MOM : 0.5, Z_VAL : -0.568524181842804\n",
            "\n",
            " LR : 0.10000000149011612, MOM : 0.8, Z_VAL : -0.5985245704650879\n",
            "\n",
            " LR : 0.10000000149011612, MOM : 0.9, Z_VAL : -0.6157993078231812\n",
            "\n",
            " LR : 0.10000000149011612, MOM : 0.99, Z_VAL : -0.6306883692741394\n",
            "\n",
            "\n",
            " Optimum :  -0.63068837  Momentum : 0.99, LR : 0.10000000149011612\n"
          ]
        }
      ],
      "source": [
        "#learn_rate = np.arange( 0.1, 0.9, 0.1, dtype = 'float32' )\n",
        "learn_rate = np.array( [0.001, 0.01, 0.05, 0.07, 0.1], dtype = 'float32' )\n",
        "momentum_set = np.array( [0.1, 0.5, 0.8, 0.9, 0.99] )\n",
        "opt_val = 1e20\n",
        "l0,m0 = 0,0\n",
        "for lr in learn_rate:\n",
        "    for mom in momentum_set:\n",
        "        w1, b1 = Weights[0][0], Weights[0][1]\n",
        "        w2, b2 = Weights[1][0], Weights[1][1]\n",
        "        lamda1 = tf.Variable(lamda_init1, dtype = tf.float32)\n",
        "        lamda2 = tf.Variable(lamda_init2, dtype = tf.float32)\n",
        "\n",
        "        wt, val= Gradient_Descent( w1,b1,w2,b2,lamda1,lamda2,Lamda_sample, Phi_sample, R_val = 2, Mu_val = 2,  momentum = mom, learning_rate = lr )\n",
        "        print(\"\\n LR : {}, MOM : {}, Z_VAL : {}\".format(lr,mom,val))\n",
        "       \n",
        "        if val <= opt_val:\n",
        "            opt_val = val\n",
        "            l0,m0 = lr,mom\n",
        "print(\"\\n\\n Optimum : \",  opt_val, \" Momentum : {}, LR : {}\".format(m0,l0))"
      ],
      "id": "CtLZ4_yjSfrY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVazG2rITMpI"
      },
      "source": [
        "## RUN "
      ],
      "id": "fVazG2rITMpI"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d121adbb",
        "outputId": "44a79037-4d24-4db0-d153-02fa7d312d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.5918502], shape=(1,), dtype=float32)  Lambda =  -0.29624262 -0.04623075  Train_Loss : 0.023474572226405144, Val_Loss : 0.05139300227165222, Test_Loss : 0.3622158467769623\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.534537], shape=(1,), dtype=float32)  Lambda =  -0.2704756 -0.046648607  Train_Loss : 0.025613021105527878, Val_Loss : 0.04603128507733345, Test_Loss : 0.36208584904670715\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.01289655], shape=(1,), dtype=float32)  Lambda =  -0.11277318 -0.51601434  Train_Loss : 0.029833929613232613, Val_Loss : 0.0456974059343338, Test_Loss : 0.36523330211639404\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.02059937], shape=(1,), dtype=float32)  Lambda =  0.15151605 -0.98363227  Train_Loss : 0.06993278861045837, Val_Loss : 0.07113752514123917, Test_Loss : 0.4160999059677124\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.01972865], shape=(1,), dtype=float32)  Lambda =  0.5822065 -1.4979033  Train_Loss : 0.06208566203713417, Val_Loss : 0.0704447329044342, Test_Loss : 0.40803951025009155\n",
            "\n",
            "\n",
            " Optimal_Lambda =  -0.11277318 -0.51601434 \n",
            "\n",
            "RUN 0 :: Validation Loss : 0.0456974059343338, Training Loss : 0.029833929613232613, Testing Loss : 0.36523330211639404\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.5943539], shape=(1,), dtype=float32)  Lambda =  -0.2845769 -0.03440148  Train_Loss : 0.024444401264190674, Val_Loss : 0.05278488248586655, Test_Loss : 0.36331573128700256\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.5266741], shape=(1,), dtype=float32)  Lambda =  -0.27097133 -0.037097394  Train_Loss : 0.02723701484501362, Val_Loss : 0.05026771500706673, Test_Loss : 0.3660498559474945\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.06951146], shape=(1,), dtype=float32)  Lambda =  -0.74390084 0.1344161  Train_Loss : 0.0314449705183506, Val_Loss : 0.05024905130267143, Test_Loss : 0.3681454062461853\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.23240358], shape=(1,), dtype=float32)  Lambda =  -0.37288898 0.29892465  Train_Loss : 0.03180946782231331, Val_Loss : 0.039072185754776, Test_Loss : 0.36496034264564514\n",
            "\n",
            "\n",
            " Violation =  tf.Tensor([0.07767053], shape=(1,), dtype=float32)  Lambda =  -0.35864887 0.9454471  Train_Loss : 0.060579001903533936, Val_Loss : 0.04958592727780342, Test_Loss : 0.39707186818122864\n",
            "\n",
            "\n",
            " Optimal_Lambda =  -0.74390084 0.1344161 \n",
            "\n",
            "RUN 1 :: Validation Loss : 0.05024905130267143, Training Loss : 0.0314449705183506, Testing Loss : 0.3681454062461853\n"
          ]
        }
      ],
      "source": [
        "# ===========FINDING LOSS=======================\n",
        "#w1_f,b1_f,w2_f,b2_f,lamda_f\n",
        "\n",
        "for run in range(2):\n",
        "\n",
        "    w1, b1 = Weights[0][0], Weights[0][1]\n",
        "    w2, b2 = Weights[1][0], Weights[1][1]\n",
        "    lamda1 = tf.Variable(lamda_init1, dtype = tf.float32)\n",
        "    lamda2 = tf.Variable(lamda_init2, dtype = tf.float32)\n",
        "\n",
        "    wt_final, lamda_final1, lamda_final2 = Augmented_Lagrangian( w1, b1, w2, b2, lamda1, lamda2, Lamda_sample, Phi_sample, m0, l0 )\n",
        "    Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n",
        "\n",
        "    model.layers[1].set_weights(Final_weights_1)\n",
        "    model.layers[2].set_weights(Final_weights_2)\n",
        "\n",
        "    val_logits = model(x_val)\n",
        "    val_loss = loss_fn(y_val,val_logits)\n",
        "\n",
        "    train_logits = model(x_train)\n",
        "    training_loss = loss_fn(y_train,train_logits)\n",
        "\n",
        "    test_logits = model(x_test)\n",
        "    test_loss = loss_fn(y_test,test_logits)\n",
        "\n",
        "    print(\"\\n\\n Optimal_Lambda = \", lamda_final1.numpy()[0],lamda_final2.numpy()[0], \"\\n\")\n",
        "    print(f'RUN {run} :: Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"
      ],
      "id": "d121adbb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8d68b5a"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "b8d68b5a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba44d87a"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ba44d87a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28827f86"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "28827f86"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e9ac5cb"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "7e9ac5cb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9a5Am8DgWk-"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "i9a5Am8DgWk-"
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "GBHO_1K_(2HP).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}