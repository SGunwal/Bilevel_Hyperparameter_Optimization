{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ceda8cd2"},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import preprocessing\n","from keras.utils.np_utils import to_categorical\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"id":"ceda8cd2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ob7JOCK9szUQ"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"id":"ob7JOCK9szUQ"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"951d09f2","outputId":"5ab410b2-1713-4815-e77e-b98f0398ee91"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3000, 784), (3000,), (2000, 784), (2000,), (10010, 784), (10010,))"]},"metadata":{},"execution_count":22}],"source":["train_dataset = np.load('train_dataset_5k_60.npz')\n","val_dataset = np.load('validation_dataset_5k_40.npz')\n","test_dataset = np.load('test_dataset.npz')\n","\n","# =========== Loading Datasets ===============\n","\n","\n","x_train = train_dataset['x'].reshape(3000, 784).astype(\"float32\") / 255\n","y_train = train_dataset['y'].astype(\"float32\")\n","  \n","x_val = val_dataset['x'].reshape(2000, 784).astype(\"float32\") / 255\n","y_val = val_dataset['y'].astype(\"float32\")   \n","                    \n","x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n","y_test = test_dataset['y'].astype(\"float32\")                    \n","\n","\n","x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "],"id":"951d09f2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"afefa986"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(784,)),\n","    tf.keras.layers.Dense(100, activation='relu'),\n","    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n","])\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"],"id":"afefa986"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cef9ecb3"},"outputs":[],"source":["@tf.function\n","def loss(w1,w2,lamda,loss_fn,y_train,logits): # Lambda\n","    total_loss = loss_fn(y_train,logits)\n","    return total_loss + tf.math.exp(lamda)*(tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2))/(2*y_train.shape[0])\n"],"id":"cef9ecb3"},{"cell_type":"markdown","metadata":{"id":"ba4b12ef"},"source":["#### $\\phi(\\lambda) = min\\{l(w;S^T) + \\exp{\\lambda}(\\|w\\|_2^2)\\}$ for given $\\lambda$"],"id":"ba4b12ef"},{"cell_type":"code","execution_count":null,"metadata":{"id":"48045099"},"outputs":[],"source":["wt_layer1_init = model.layers[1].get_weights()\n","wt_layer2_init = model.layers[2].get_weights()\n","\n","train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","train_df = train_df.shuffle(buffer_size = 3000).batch(64)\n","\n","def fmin_loss(lamda, epochs= 50, l_rate= 0.6, momentum= 0.01, t_hold = 1e-6, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n","    tf.keras.backend.clear_session()\n","    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n","    total_loss0 = 1e20\n","\n","    for epoch in range(epochs):\n","        for step,(x_train,y_train) in enumerate(train_df):\n","            with tf.GradientTape() as tape:\n","                logits = model(x_train, training=True)\n","                w1 = model.layers[1].weights[0]\n","                w2 = model.layers[2].weights[0]\n","                total_loss1 = loss(w1,w2,lamda,loss_fn,y_train,logits)\n","                \n","            vars_list = model.trainable_weights\n","            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n","            optimizer.apply_gradients(zip(grads,vars_list))\n","\n","        # if abs( total_loss1-total_loss0 ) <= t_hold:\n","        #     break\n","        total_loss0 = total_loss1\n","\n","    wt_layer1 = model.layers[1].get_weights()\n","    wt_layer2 = model.layers[2].get_weights()\n","    model.layers[1].set_weights(wt_layer1_init)\n","    model.layers[2].set_weights(wt_layer2_init)\n","\n","    return [total_loss1, wt_layer1, wt_layer2]"],"id":"48045099"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c176e652"},"outputs":[],"source":["def f_val(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_train)\n","    return loss(w1,w2,lamda,loss_fn,y_train,logits)\n","\n","def f_grad(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    with tf.GradientTape() as tape:\n","        total_loss = f_val(w1,b1,w2,b2,lamda)\n","    vars_list = model.trainable_weights\n","    vars_list.append(lamda)\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads\n","\n","def F_val(w1,b1,w2,b2):     \n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_val)\n","    return loss_fn(y_val,logits)\n","\n","def F_grad(w1,b1,w2,b2):   # float32 arrays\n","    with tf.GradientTape() as tape:\n","        total_loss = F_val(w1,b1,w2,b2)\n","    vars_list = model.trainable_weights\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads"],"id":"c176e652"},{"cell_type":"markdown","metadata":{"id":"e33981fc"},"source":["### Gaussian Process Regressor"],"id":"e33981fc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f994b19c"},"outputs":[],"source":["from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF\n","import matplotlib.pyplot as plt\n","\n","def GPR( Lamda_sample, Phi_sample, lamda ):\n","\n","    kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e4))\n","    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n","    gaussian_process.fit(Lamda_sample, Phi_sample)\n","\n","    k2_l = gaussian_process.kernel_.get_params()['k2__length_scale']\n","    x = lamda\n","    y_pred, sigma = gaussian_process.predict(x, return_std=True)\n","    y_pred_grad = 0.0*y_pred\n","\n","    for key, x_star in enumerate(x):\n","\n","        k_val=gaussian_process.kernel_( Lamda_sample , np.atleast_2d(x_star) , eval_gradient=False).ravel() \n","        x_diff_over_l_sq = ((Lamda_sample-x_star)/np.power(k2_l,2)).ravel()\n","        intermediate_result = np.multiply(k_val, x_diff_over_l_sq)\n","        final_result = np.dot(intermediate_result, gaussian_process.alpha_)\n","        y_pred_grad[key] = final_result\n","        \n","    return [y_pred[0], y_pred_grad[0]]"],"id":"f994b19c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5394a874"},"outputs":[],"source":["def Function_Grad( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","\n","    Phi = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    Phi_val, Phi_grad = Phi[0], Phi[1]\n","    \n","    p1 = F_grad( w1,b1,w2,b2 ) \n","    p2 = ( R_val* ( f_val(w1,b1,w2,b2,lamda) -  Phi_val) + Mu_val )\n","    f_gradient = f_grad(w1,b1,w2,b2,lamda)\n","    p3 = f_gradient[:-1] + [f_gradient[-1]-Phi_grad]\n","    p2tp3 = [p2*elt for elt in p3]\n","\n","    gradients = [e1+e2 for e1,e2 in zip(p1,p2tp3[:-1])] + [p2tp3[-1]]\n","    gradients = [(tf.clip_by_norm(grad, clip_norm = 2.0)) for grad in gradients]\n","\n","    return gradients   # Output : w1, b1, w2, b2, lamda : floar32 arrays"],"id":"5394a874"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9afbbbe3"},"outputs":[],"source":["def AugLag_Function( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","    PHI = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    f_phi = f_val(w1,b1,w2,b2,lamda) - PHI[0]\n","    final_val = F_val(w1,b1,w2,b2) + (R_val/2) * f_phi**2 + Mu_val * f_phi\n","    return final_val.numpy()[0]"],"id":"9afbbbe3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"10c95548"},"outputs":[],"source":["def Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val, Mu_val, momentum, learning_rate, \n","                     epochs = 80, Opt_tol = 1e-10, F_tol = 1e-6 ): # With Nesterov Momentum\n","\n","    w1_update0 = np.zeros((784,100),dtype='float32')\n","    b1_update0 = np.zeros((100),dtype='float32')\n","    w2_update0 = np.zeros((100,10),dtype='float32')\n","    b2_update0 = np.zeros((10),dtype='float32')\n","    lamda_update0 = np.zeros((1),dtype='float32')\n","\n","    w1_weight0, w2_weight0, b1_weight0, b2_weight0, lamda_weight0 = w1, w2, b1, b2, lamda  # Initialized weights\n","    Loss_val = 1e+20\n","    #Z_VAL0 = 1e+20\n","    Opt_weights = []\n","    \n","    for epoch in range(epochs):\n","\n","        w1_weight_ahead = w1_weight0 - momentum*w1_update0\n","        b1_weight_ahead = b1_weight0 - momentum*b1_update0\n","        w2_weight_ahead = w2_weight0 - momentum*w2_update0\n","        b2_weight_ahead = b2_weight0 - momentum*b2_update0\n","\n","        lamda_weight_ahead = lamda_weight0 - momentum*lamda_update0\n","        lamda_weight_ahead = tf.Variable(lamda_weight_ahead.numpy(), dtype = tf.float32)\n","\n","        Grad_list = Function_Grad( w1_weight_ahead, b1_weight_ahead, w2_weight_ahead, b2_weight_ahead, \n","                                  lamda_weight_ahead, Lamda_sample, Phi_sample, R_val, Mu_val )\n","\n","        learning_rate = tf.constant(learning_rate, dtype=tf.float32)\n","        momentum = tf.constant(momentum, dtype=tf.float32)\n","\n","        part1 = [momentum*update for update in [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0]]\n","        part2 = [learning_rate * grad for grad in Grad_list]\n","\n","        [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0] = [i+j for i,j in zip(part1,part2)]  \n","        [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0] = np.subtract([w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0], \n","                                                                                      [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0])\n","        \n","        Z_VAL = AugLag_Function( w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0,\n","                                Lamda_sample, Phi_sample, R_val, Mu_val )\n","        Optimality_Tolerance = np.array([np.linalg.norm( optgd, ord = np.inf ) for optgd in Grad_list])\n","\n","        if Z_VAL <= Loss_val :\n","            \n","            Opt_weights = [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0]\n","            fun_tol = abs( (Z_VAL-Loss_val)/(1+abs(Loss_val)) )\n","            \n","            if  fun_tol <= F_tol and np.all(Optimality_Tolerance) <= Opt_tol:\n","                Loss_val = Z_VAL\n","                break\n","                \n","            Loss_val = Z_VAL\n","        #=====================================================================================\n","\n","    return Opt_weights, Loss_val   # w1, b1, w2, b2, lamda\n"],"id":"10c95548"},{"cell_type":"code","source":["def LOSS_FUNCTION( wts1, wts2 ):\n","  \n","  model.layers[1].set_weights(wts1)\n","  model.layers[2].set_weights(wts2)\n","\n","  val_logits = model(x_val)\n","  val_loss = loss_fn(y_val,val_logits)\n","\n","  train_logits = model(x_train)\n","  training_loss = loss_fn(y_train,train_logits)\n","\n","  test_logits = model(x_test)\n","  test_loss = loss_fn(y_test,test_logits)\n","\n","  return val_loss, training_loss, test_loss"],"metadata":{"id":"J4nXC7cOL5m2"},"id":"J4nXC7cOL5m2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bad8b81"},"outputs":[],"source":["# GLOBAL : Lamda_sample, Phi_sample\n","def Augmented_Lagrangian( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, mom, lr, R_val = 2,\n","                         Mu_val = 2, neta = 1.5, al_epochs = 5 ):\n","    Z_val0 = 1e+20\n","    wts_opt0, lamda_opt0, min_violation = [], 0, 10\n","    for epoch in range(al_epochs):\n","\n","        Opt_Weights, _ = Gradient_Descent( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val, mom, lr)\n","        \n","        w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt = Opt_Weights[0], Opt_Weights[1], Opt_Weights[2], Opt_Weights[3], Opt_Weights[4]  # lamda not var\n","        \n","        valL, trL, testL = LOSS_FUNCTION( [w1_opt,b1_opt], [w2_opt,b2_opt] )\n","\n","        # ========== Setting Weights for Next Loop ====================\n","        #w1,b1,w2,b2,lamda = w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt\n","        \n","        # ============== Updating Augmented Lagrangian Parameters ====================\n","        Phi1 = GPR(Lamda_sample, Phi_sample, np.array([[lamda_opt.numpy()[0]]]))\n","        Phi_val1 = Phi1[0]\n","        lamda_opt = tf.Variable(lamda_opt, dtype=tf.float32)                   # Converting lamda to var\n","        constraint_ = f_val(w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt) - Phi_val1\n","        Mu_val = Mu_val + R_val*( constraint_ )\n","        R_val  = neta*R_val\n","\n","        # ===== Constraint Violation Criteria ===================\n","        violation = abs(constraint_)\n","        print(\"\\n\\n Violation = \", violation,\"Lambda = \", lamda_opt.numpy()[0], f\" Train_Loss : {trL}, Val_Loss : {valL}, Test_Loss : {testL}\")\n","        # if violation <= min_violation:\n","        wts_opt0 = [ [w1_opt, b1_opt], [w2_opt, b2_opt]]\n","        #   lamda_opt0 = lamda_opt\n","        #   min_violation = violation\n","        \n","        # ==========UPDATING LAMBDA SAMPLE FOR GPR=========================\n","        Lamda_sample = np.sort(np.array(np.vstack([Lamda_sample,[lamda_opt.numpy()]]), dtype = 'float32'), axis = 0)\n","        index = np.where(Lamda_sample == [lamda_opt.numpy()[0]] )\n","        Phi_new_lamda = fmin_loss(lamda_opt)[0].numpy()[0]\n","        Phi_sample = np.insert(Phi_sample, index[0][0], Phi_new_lamda)\n","      \n","    print(f\"\\n Optimal Lambda = {lamda_opt.numpy()[0]}\")\n","    return wts_opt0, lamda_opt #, min_violation"],"id":"1bad8b81"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cbc32ef","outputId":"e00312be-3930-4e0f-e3ec-5bd4dcd6d342"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Phi_sample:  [0.001465   0.00075194 0.00118724 0.00223559 0.00560613 0.01316278\n"," 0.02719149 0.0542752  0.1090351  0.23284437] \n","\n","\n"]}],"source":["Lamda_sample = np.arange(-10,0,1, dtype = 'float32').reshape((-1,1))\n","\n","Phi_sample, Weights, lamda_init = [], [], 0\n","init_val_loss = 1e20\n","for lamda in Lamda_sample:\n","    \n","    [min_loss, layer1wt, layer2wt] = fmin_loss(lamda)\n","    Phi_sample.append( min_loss.numpy()[0] )\n","    \n","    model.layers[1].set_weights(layer1wt)\n","    model.layers[2].set_weights(layer2wt)\n","    val_logits = model(x_val)\n","    val_loss = loss_fn(y_val,val_logits)\n","    if val_loss <= init_val_loss :\n","        init_val_loss = val_loss\n","        Weights = [layer1wt, layer2wt]\n","        lamda_init = lamda\n","        \n","Phi_sample = np.array(Phi_sample)\n","print(\"\\n\\n Phi_sample: \", Phi_sample, \"\\n\\n\")\n","\n","#============================== Initializing Weights =================================================\n","# w1, b1 = Weights[0][0], Weights[0][1]\n","# w2, b2 = Weights[1][0], Weights[1][1]\n","# lamda = tf.Variable(lamda_init, dtype = tf.float32)\n"],"id":"5cbc32ef"},{"cell_type":"markdown","metadata":{"id":"C9VlZi8tioAR"},"source":["### Tuning Gradient Descent"],"id":"C9VlZi8tioAR"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqZxVpgtinao","outputId":"d915aa78-dcfc-406d-9c14-aa2c9530c9ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," LR : 0.0010000000474974513, MOM : 0.1, Z_VAL : 0.09269604086875916\n","\n"," LR : 0.0010000000474974513, MOM : 0.5, Z_VAL : 0.08027750253677368\n","\n"," LR : 0.0010000000474974513, MOM : 0.8, Z_VAL : 0.04953892529010773\n","\n"," LR : 0.0010000000474974513, MOM : 0.9, Z_VAL : 0.003701522946357727\n","\n"," LR : 0.0010000000474974513, MOM : 0.99, Z_VAL : -0.6227986216545105\n","\n"," LR : 0.009999999776482582, MOM : 0.1, Z_VAL : -0.012344911694526672\n","\n"," LR : 0.009999999776482582, MOM : 0.5, Z_VAL : -0.11443436145782471\n","\n"," LR : 0.009999999776482582, MOM : 0.8, Z_VAL : -0.6037972569465637\n","\n"," LR : 0.009999999776482582, MOM : 0.9, Z_VAL : -0.8731462955474854\n","\n"," LR : 0.009999999776482582, MOM : 0.99, Z_VAL : -0.9879288077354431\n","\n"," LR : 0.05000000074505806, MOM : 0.1, Z_VAL : -0.7142205834388733\n","\n"," LR : 0.05000000074505806, MOM : 0.5, Z_VAL : -0.8715137243270874\n","\n"," LR : 0.05000000074505806, MOM : 0.8, Z_VAL : -0.9306536316871643\n","\n"," LR : 0.05000000074505806, MOM : 0.9, Z_VAL : -0.9566768407821655\n","\n"," LR : 0.05000000074505806, MOM : 0.99, Z_VAL : -0.994482696056366\n","\n"," LR : 0.07000000029802322, MOM : 0.1, Z_VAL : -0.8354514241218567\n","\n"," LR : 0.07000000029802322, MOM : 0.5, Z_VAL : -0.9015809297561646\n","\n"," LR : 0.07000000029802322, MOM : 0.8, Z_VAL : -0.9432843327522278\n","\n"," LR : 0.07000000029802322, MOM : 0.9, Z_VAL : -0.9682297110557556\n","\n"," LR : 0.07000000029802322, MOM : 0.99, Z_VAL : -0.9921963214874268\n","\n"," LR : 0.10000000149011612, MOM : 0.1, Z_VAL : -0.8834798336029053\n","\n"," LR : 0.10000000149011612, MOM : 0.5, Z_VAL : -0.9180800318717957\n","\n"," LR : 0.10000000149011612, MOM : 0.8, Z_VAL : -0.960598886013031\n","\n"," LR : 0.10000000149011612, MOM : 0.9, Z_VAL : -0.9811891913414001\n","\n"," LR : 0.10000000149011612, MOM : 0.99, Z_VAL : -0.9847521185874939\n","\n","\n"," Optimum :  -0.9944827  Momentum : 0.99, LR : 0.05000000074505806\n"]}],"source":["# learn_rate = np.arange( 0.1, 1, 0.1, dtype = 'float32' )\n","# momentum_set = np.arange( 0.01, 0.1, 0.01 )\n","learn_rate = np.array( [0.001, 0.01, 0.05, 0.07, 0.1], dtype = 'float32' )\n","momentum_set = np.array( [0.1, 0.5, 0.8, 0.9, 0.99] )\n","\n","opt_val = 1e20\n","l0,m0 = 0,0\n","for lr in learn_rate:\n","    for mom in momentum_set:\n","        w1, b1 = Weights[0][0], Weights[0][1]\n","        w2, b2 = Weights[1][0], Weights[1][1]\n","        lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","        wt, val = Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val = 2, Mu_val = 2,\n","                                   momentum = mom, learning_rate = lr )\n","        print(\"\\n LR : {}, MOM : {}, Z_VAL : {}\".format(lr,mom,val))\n","        \n","        if val <= opt_val:\n","            opt_val = val\n","            l0,m0 = lr,mom\n","print(\"\\n\\n Optimum : \",  opt_val, \" Momentum : {}, LR : {}\".format(m0,l0))"],"id":"rqZxVpgtinao"},{"cell_type":"markdown","source":["## LLO = 10 (+5)"],"metadata":{"id":"tQ6U0lzHUvM7"},"id":"tQ6U0lzHUvM7"},{"cell_type":"markdown","source":["###### Note : The result might differ a little because of GPR estimation every time the program is executed. Training the program to its optimal is avoided because it fits the model over training and validation dataset too well, leading to overfitting on validation dataset. "],"metadata":{"id":"JXe716p2Xfbq"},"id":"JXe716p2Xfbq"},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, m0, l0 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXdXC9ueLt4K","outputId":"a85c838e-5556-4dc7-c58d-2b9f786dbc8a"},"id":"EXdXC9ueLt4K","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.5033908], shape=(1,), dtype=float32) Lambda =  1.0961769  Train_Loss : 0.012343702837824821, Val_Loss : 0.009152368642389774, Test_Loss : 0.23267892003059387\n","\n","\n"," Violation =  tf.Tensor([0.321246], shape=(1,), dtype=float32) Lambda =  -0.1448857  Train_Loss : 0.11168576776981354, Val_Loss : 0.014085515402257442, Test_Loss : 0.26525402069091797\n","\n","\n"," Violation =  tf.Tensor([0.02682782], shape=(1,), dtype=float32) Lambda =  -2.8429246  Train_Loss : 0.08575254678726196, Val_Loss : 0.014549980871379375, Test_Loss : 0.25131815671920776\n","\n","\n"," Violation =  tf.Tensor([0.00323461], shape=(1,), dtype=float32) Lambda =  -3.279241  Train_Loss : 0.06032222509384155, Val_Loss : 0.013646169565618038, Test_Loss : 0.24216175079345703\n","\n","\n"," Violation =  tf.Tensor([0.0017165], shape=(1,), dtype=float32) Lambda =  -2.7528846  Train_Loss : 0.04832477495074272, Val_Loss : 0.013327003456652164, Test_Loss : 0.23835283517837524\n","\n"," Optimal Lambda = -2.75288462638855\n","Validation Loss : 0.013327003456652164, Training Loss : 0.04832477495074272, Testing Loss : 0.23835283517837524\n"]}]},{"cell_type":"markdown","source":["## RUN 2"],"metadata":{"id":"UAjcrUjlUqtn"},"id":"UAjcrUjlUqtn"},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, m0, l0 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"id":"1tunQ5RKMFfa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b5aef279-8198-4af4-a663-1def67b2a763"},"id":"1tunQ5RKMFfa","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.5031711], shape=(1,), dtype=float32) Lambda =  1.0691944  Train_Loss : 0.012894687242805958, Val_Loss : 0.009143189527094364, Test_Loss : 0.2330009788274765\n","\n","\n"," Violation =  tf.Tensor([0.2709697], shape=(1,), dtype=float32) Lambda =  -0.04971412  Train_Loss : 0.1630280315876007, Val_Loss : 0.016089271754026413, Test_Loss : 0.291731059551239\n","\n","\n"," Violation =  tf.Tensor([0.02196291], shape=(1,), dtype=float32) Lambda =  -2.0303788  Train_Loss : 0.08207900822162628, Val_Loss : 0.014001564122736454, Test_Loss : 0.250826895236969\n","\n","\n"," Violation =  tf.Tensor([0.01223473], shape=(1,), dtype=float32) Lambda =  -3.7565382  Train_Loss : 0.07040217518806458, Val_Loss : 0.013373413123190403, Test_Loss : 0.2454884648323059\n","\n","\n"," Violation =  tf.Tensor([0.00721183], shape=(1,), dtype=float32) Lambda =  -3.5506096  Train_Loss : 0.04117424413561821, Val_Loss : 0.013605846092104912, Test_Loss : 0.2344062179327011\n","\n"," Optimal Lambda = -3.550609588623047\n","Validation Loss : 0.013605846092104912, Training Loss : 0.04117424413561821, Testing Loss : 0.2344062179327011\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"hlgRM3obUoio"},"id":"hlgRM3obUoio","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"GBHO_5K_1HP_.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}