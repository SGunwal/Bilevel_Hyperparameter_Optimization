{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oA7JgX6M2lnW"},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import preprocessing\n","from keras.utils.np_utils import to_categorical\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZkjQU6sl228l"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5Y_PA0lD3gb","outputId":"bcf8de43-c323-4dee-c201-fae0b5355560"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((6000, 784), (6000,), (4000, 784), (4000,), (10010, 784), (10010,))"]},"metadata":{},"execution_count":29}],"source":["train_dataset = np.load('train_dataset_10k_60.npz')\n","val_dataset = np.load('validation_dataset_10k_40.npz')\n","test_dataset = np.load('test_dataset.npz')\n","\n","# =========== Loading Datasets ===============\n","\n","\n","x_train = train_dataset['x'].reshape(6000, 784).astype(\"float32\") / 255\n","y_train = train_dataset['y'].astype(\"float32\")\n","  \n","x_val = val_dataset['x'].reshape(4000, 784).astype(\"float32\") / 255\n","y_val = val_dataset['y'].astype(\"float32\")   \n","                    \n","x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n","y_test = test_dataset['y'].astype(\"float32\")                    \n","\n","\n","x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hx4paNEKk_SJ"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(784,)),\n","    tf.keras.layers.Dense(100, activation='relu'),\n","    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n","])\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANg2cxFPk_PY"},"outputs":[],"source":["@tf.function\n","def loss(w1,w2,lamda,loss_fn,y_train,logits): # Lambda\n","    total_loss = loss_fn(y_train,logits)\n","    return total_loss + tf.math.exp(lamda)*(tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2))/(2*y_train.shape[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HEjb0-rbk_NC"},"outputs":[],"source":["wt_layer1_init = model.layers[1].get_weights()\n","wt_layer2_init = model.layers[2].get_weights()\n","\n","train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","train_df = train_df.shuffle(buffer_size = 750).batch(64)\n","\n","def fmin_loss(lamda, epochs= 50, l_rate= 0.2, momentum= 0.079, t_hold = 1e-6, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n","    tf.keras.backend.clear_session()\n","    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n","    total_loss0 = 1e20\n","\n","    for epoch in range(epochs):\n","        for step,(x_train,y_train) in enumerate(train_df):\n","            with tf.GradientTape() as tape:\n","                logits = model(x_train, training=True)\n","                w1 = model.layers[1].weights[0]\n","                w2 = model.layers[2].weights[0]\n","                total_loss1 = loss(w1,w2,lamda,loss_fn,y_train,logits)\n","                \n","            vars_list = model.trainable_weights\n","            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n","            optimizer.apply_gradients(zip(grads,vars_list))\n","\n","        total_loss0 = total_loss1\n","\n","    wt_layer1 = model.layers[1].get_weights()\n","    wt_layer2 = model.layers[2].get_weights()\n","    model.layers[1].set_weights(wt_layer1_init)\n","    model.layers[2].set_weights(wt_layer2_init)\n","\n","    return [total_loss1, wt_layer1, wt_layer2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxfkFCsYk_IN"},"outputs":[],"source":["def f_val(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_train)\n","    return loss(w1,w2,lamda,loss_fn,y_train,logits)\n","\n","def f_grad(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    with tf.GradientTape() as tape:\n","        total_loss = f_val(w1,b1,w2,b2,lamda)\n","    vars_list = model.trainable_weights\n","    vars_list.append(lamda)\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads\n","\n","def F_val(w1,b1,w2,b2):     \n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_val)\n","    return loss_fn(y_val,logits)\n","\n","def F_grad(w1,b1,w2,b2):   # float32 arrays\n","    with tf.GradientTape() as tape:\n","        total_loss = F_val(w1,b1,w2,b2)\n","    vars_list = model.trainable_weights\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALW8VWD1k_FW"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gnsp0WsTlSEn"},"source":["### GPR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LcuEf7gkk_Co"},"outputs":[],"source":["from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF\n","import matplotlib.pyplot as plt\n","\n","def GPR( Lamda_sample, Phi_sample, lamda ):\n","\n","    kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e4))\n","    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n","    gaussian_process.fit(Lamda_sample, Phi_sample)\n","\n","    k2_l = gaussian_process.kernel_.get_params()['k2__length_scale']\n","    x = lamda\n","    y_pred, sigma = gaussian_process.predict(x, return_std=True)\n","    y_pred_grad = 0.0*y_pred\n","\n","    for key, x_star in enumerate(x):\n","\n","        k_val=gaussian_process.kernel_( Lamda_sample , np.atleast_2d(x_star) , eval_gradient=False).ravel() \n","        x_diff_over_l_sq = ((Lamda_sample-x_star)/np.power(k2_l,2)).ravel()\n","        intermediate_result = np.multiply(k_val, x_diff_over_l_sq)\n","        final_result = np.dot(intermediate_result, gaussian_process.alpha_)\n","        y_pred_grad[key] = final_result\n","        \n","    return [y_pred[0], y_pred_grad[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DfoaqP7k-_6"},"outputs":[],"source":["def Function_Grad( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","\n","    Phi = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    Phi_val, Phi_grad = Phi[0], Phi[1]\n","    \n","    p1 = F_grad( w1,b1,w2,b2 ) \n","    p2 = ( R_val* ( f_val(w1,b1,w2,b2,lamda) -  Phi_val) + Mu_val )\n","    f_gradient = f_grad(w1,b1,w2,b2,lamda)\n","    p3 = f_gradient[:-1] + [f_gradient[-1]-Phi_grad]\n","    p2tp3 = [p2*elt for elt in p3]\n","\n","    gradients = [e1+e2 for e1,e2 in zip(p1,p2tp3[:-1])] + [p2tp3[-1]]\n","    gradients = [(tf.clip_by_norm(grad, clip_norm = 2.0)) for grad in gradients]\n","\n","    return gradients   # Output : w1, b1, w2, b2, lamda : floar32 arrays"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pspMOi51lVhH"},"outputs":[],"source":["def AugLag_Function( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","    PHI = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    f_phi = f_val(w1,b1,w2,b2,lamda) - PHI[0]\n","    final_val = F_val(w1,b1,w2,b2) + (R_val/2) * f_phi**2 + Mu_val * f_phi\n","    return final_val.numpy()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZ0cYBlTlWmd"},"outputs":[],"source":["def Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val, Mu_val, momentum, learning_rate, \n","                     epochs = 50, Opt_tol = 1e-10, F_tol = 1e-6 ): # With Nesterov Momentum\n","\n","    w1_update0 = np.zeros((784,100),dtype='float32')\n","    b1_update0 = np.zeros((100),dtype='float32')\n","    w2_update0 = np.zeros((100,10),dtype='float32')\n","    b2_update0 = np.zeros((10),dtype='float32')\n","    lamda_update0 = np.zeros((1),dtype='float32')\n","\n","    w1_weight0, w2_weight0, b1_weight0, b2_weight0, lamda_weight0 = w1, w2, b1, b2, lamda  # Initialized weights\n","    Loss_val = 1e+20\n","    Z_VAL0 = 1e+20\n","    Opt_weights = []\n","    \n","    for epoch in range(epochs):\n","\n","        w1_weight_ahead = w1_weight0 - momentum*w1_update0\n","        b1_weight_ahead = b1_weight0 - momentum*b1_update0\n","        w2_weight_ahead = w2_weight0 - momentum*w2_update0\n","        b2_weight_ahead = b2_weight0 - momentum*b2_update0\n","\n","        lamda_weight_ahead = lamda_weight0 - momentum*lamda_update0\n","        lamda_weight_ahead = tf.Variable(lamda_weight_ahead.numpy(), dtype = tf.float32)\n","\n","        Grad_list = Function_Grad( w1_weight_ahead, b1_weight_ahead, w2_weight_ahead, b2_weight_ahead, \n","                                  lamda_weight_ahead, Lamda_sample, Phi_sample, R_val, Mu_val )\n","\n","        learning_rate = tf.constant(learning_rate, dtype=tf.float32)\n","        momentum = tf.constant(momentum, dtype=tf.float32)\n","\n","        part1 = [momentum*update for update in [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0]]\n","        part2 = [learning_rate * grad for grad in Grad_list]\n","\n","        [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0] = [i+j for i,j in zip(part1,part2)]  \n","        [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0] = np.subtract([w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0], \n","                                                                                      [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0])\n","        \n","        Z_VAL = AugLag_Function( w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0,\n","                                Lamda_sample, Phi_sample, R_val, Mu_val )\n","        Optimality_Tolerance = np.array([np.linalg.norm( optgd, ord = np.inf ) for optgd in Grad_list])\n","\n","        if Z_VAL <= Loss_val :\n","            \n","            Opt_weights = [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0]\n","            fun_tol = abs( (Z_VAL-Loss_val)/(1+abs(Loss_val)) )\n","            \n","            if  fun_tol <= F_tol and np.all(Optimality_Tolerance) <= Opt_tol:\n","                Loss_val = Z_VAL\n","                break\n","                \n","            Loss_val = Z_VAL\n","        #=====================================================================================\n","\n","    return Opt_weights, Loss_val   # w1, b1, w2, b2, lamda\n"]},{"cell_type":"code","source":["def LOSS_FUNCTION( wts1, wts2 ):\n","  \n","  model.layers[1].set_weights(wts1)\n","  model.layers[2].set_weights(wts2)\n","\n","  val_logits = model(x_val)\n","  val_loss = loss_fn(y_val,val_logits)\n","\n","  train_logits = model(x_train)\n","  training_loss = loss_fn(y_train,train_logits)\n","\n","  test_logits = model(x_test)\n","  test_loss = loss_fn(y_test,test_logits)\n","\n","  return val_loss, training_loss, test_loss"],"metadata":{"id":"20FSdvkiL6dW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mt7qKWEDlVeU"},"outputs":[],"source":["# GLOBAL : Lamda_sample, Phi_sample\n","def Augmented_Lagrangian( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, mom, lr, R_val = 2,\n","                         Mu_val = 2, neta = 1.5, al_epochs = 5 ):\n","    Z_val0 = 1e+20\n","    wts_opt0, lamda_opt0, min_violation = [], 0, 10\n","    for epoch in range(al_epochs):\n","\n","        Opt_Weights, _ = Gradient_Descent( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val, mom, lr)\n","        \n","        w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt = Opt_Weights[0], Opt_Weights[1], Opt_Weights[2], Opt_Weights[3], Opt_Weights[4]  # lamda not var\n","        \n","        valL, trL, testL = LOSS_FUNCTION( [w1_opt,b1_opt], [w2_opt,b2_opt] )\n","\n","        # ========== Setting Weights for Next Loop ====================\n","        #w1,b1,w2,b2,lamda = w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt\n","        \n","        # ============== Updating Augmented Lagrangian Parameters ====================\n","        Phi1 = GPR(Lamda_sample, Phi_sample, np.array([[lamda_opt.numpy()[0]]]))\n","        Phi_val1 = Phi1[0]\n","        lamda_opt = tf.Variable(lamda_opt, dtype=tf.float32)                   # Converting lamda to var\n","        constraint_ = f_val(w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt) - Phi_val1\n","        Mu_val = Mu_val + R_val*( constraint_ )\n","        R_val  = neta*R_val\n","\n","        # ===== Constraint Violation Criteria ===================\n","        violation = abs(constraint_)\n","        print(\"\\n\\n Violation = \", violation, \" Lambda = \", lamda_opt.numpy()[0], f\" Train_Loss : {trL}, Val_Loss : {valL}, Test_Loss : {testL}\")\n","        if violation <= min_violation:\n","          wts_opt0 = [ [w1_opt, b1_opt], [w2_opt, b2_opt] ]\n","          lamda_opt0 = lamda_opt\n","          min_violation = violation\n","        \n","        # ==========UPDATING LAMBDA SAMPLE FOR GPR=========================\n","        Lamda_sample = np.sort(np.array(np.vstack([Lamda_sample,[lamda_opt.numpy()]]), dtype = 'float32'), axis = 0)\n","        index = np.where(Lamda_sample == [lamda_opt.numpy()[0]] )\n","        Phi_new_lamda = fmin_loss(lamda_opt)[0].numpy()[0]\n","        Phi_sample = np.insert(Phi_sample, index[0][0], Phi_new_lamda)\n","      \n","    return wts_opt0, lamda_opt0 #, min_violation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmxFUrYOlVbf","outputId":"7529b50b-9826-4ff2-cee7-93c9155f0a40"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Phi_sample:  [0.00377333 0.00186253 0.00201149 0.00309655 0.00741266 0.0183509\n"," 0.03982027 0.07663073 0.13814265 0.31707436] \n","\n","\n"]}],"source":["Lamda_sample = np.arange(-10,0,1, dtype = 'float32').reshape((-1,1))\n","\n","Phi_sample, Weights, lamda_init = [], [], 0\n","init_val_loss = 1e20\n","for lamda in Lamda_sample:\n","    \n","    [min_loss, layer1wt, layer2wt] = fmin_loss(lamda)\n","    Phi_sample.append( min_loss.numpy()[0] )\n","    \n","    model.layers[1].set_weights(layer1wt)\n","    model.layers[2].set_weights(layer2wt)\n","    val_logits = model(x_val)\n","    val_loss = loss_fn(y_val,val_logits)\n","    if val_loss <= init_val_loss :\n","        init_val_loss = val_loss\n","        Weights = [layer1wt, layer2wt]\n","        lamda_init = lamda\n","        \n","Phi_sample = np.array(Phi_sample)\n","print(\"\\n\\n Phi_sample: \", Phi_sample, \"\\n\\n\")\n","\n","#============================== Initializing Weights =================================================\n","# w1, b1 = Weights[0][0], Weights[0][1]\n","# w2, b2 = Weights[1][0], Weights[1][1]\n","# lamda = tf.Variable(lamda_init, dtype = tf.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIwKHAwdldsi","outputId":"2324b691-1bcc-44bd-d658-bdd6fa20b94d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," LR : 0.0010000000474974513, MOM : 0.1, Z_VAL : -0.07093600928783417\n","\n"," LR : 0.0010000000474974513, MOM : 0.5, Z_VAL : -0.0761214941740036\n","\n"," LR : 0.0010000000474974513, MOM : 0.8, Z_VAL : -0.09148558974266052\n","\n"," LR : 0.0010000000474974513, MOM : 0.9, Z_VAL : -0.11584392189979553\n","\n"," LR : 0.0010000000474974513, MOM : 0.99, Z_VAL : -0.3224531412124634\n","\n"," LR : 0.009999999776482582, MOM : 0.1, Z_VAL : -0.12931270897388458\n","\n"," LR : 0.009999999776482582, MOM : 0.5, Z_VAL : -0.19685828685760498\n","\n"," LR : 0.009999999776482582, MOM : 0.8, Z_VAL : -0.4801451563835144\n","\n"," LR : 0.009999999776482582, MOM : 0.9, Z_VAL : -0.6305465698242188\n","\n"," LR : 0.009999999776482582, MOM : 0.99, Z_VAL : -0.6744570732116699\n","\n"," LR : 0.05000000074505806, MOM : 0.1, Z_VAL : -0.5350652933120728\n","\n"," LR : 0.05000000074505806, MOM : 0.5, Z_VAL : -0.6309939026832581\n","\n"," LR : 0.05000000074505806, MOM : 0.8, Z_VAL : -0.66331946849823\n","\n"," LR : 0.05000000074505806, MOM : 0.9, Z_VAL : -0.688239336013794\n","\n"," LR : 0.05000000074505806, MOM : 0.99, Z_VAL : -0.7391195297241211\n","\n"," LR : 0.07000000029802322, MOM : 0.1, Z_VAL : -0.6082594394683838\n","\n"," LR : 0.07000000029802322, MOM : 0.5, Z_VAL : -0.6452361345291138\n","\n"," LR : 0.07000000029802322, MOM : 0.8, Z_VAL : -0.6755390167236328\n","\n"," LR : 0.07000000029802322, MOM : 0.9, Z_VAL : -0.7018473148345947\n","\n"," LR : 0.07000000029802322, MOM : 0.99, Z_VAL : -0.7413535118103027\n","\n"," LR : 0.10000000149011612, MOM : 0.1, Z_VAL : -0.6367943286895752\n","\n"," LR : 0.10000000149011612, MOM : 0.5, Z_VAL : -0.6564940214157104\n","\n"," LR : 0.10000000149011612, MOM : 0.8, Z_VAL : -0.6894180774688721\n","\n"," LR : 0.10000000149011612, MOM : 0.9, Z_VAL : -0.7155663371086121\n","\n"," LR : 0.10000000149011612, MOM : 0.99, Z_VAL : -0.7454214096069336\n","\n","\n"," Optimum :  -0.7454214  Momentum : 0.99, LR : 0.10000000149011612\n"]}],"source":["learn_rate = np.array( [0.001, 0.01, 0.05, 0.07, 0.1], dtype = 'float32' )\n","momentum_set = np.array( [0.1, 0.5, 0.8, 0.9, 0.99] )\n","\n","opt_val = 1e20\n","l0,m0 = 0,0\n","for lr in learn_rate:\n","    for mom in momentum_set:\n","        w1, b1 = Weights[0][0], Weights[0][1]\n","        w2, b2 = Weights[1][0], Weights[1][1]\n","        lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","        wt, val = Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val = 2, Mu_val = 2,\n","                                   momentum = mom, learning_rate = lr )\n","        print(\"\\n LR : {}, MOM : {}, Z_VAL : {}\".format(lr,mom,val))\n","        \n","        if val <= opt_val:\n","            opt_val = val\n","            l0,m0 = lr,mom\n","print(\"\\n\\n Optimum : \",  opt_val, \" Momentum : {}, LR : {}\".format(m0,l0))"]},{"cell_type":"markdown","source":["## LLO = 10(+5)"],"metadata":{"id":"ODmsCjwO-mv8"}},{"cell_type":"markdown","source":["###### Note : The result might differ a little because of GPR estimation every time the program is executed. Training the program to its optimal is avoided because it fits the model over training and validation dataset too well, leading to overfitting on validation dataset. "],"metadata":{"id":"g3lMCkn--g3f"}},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, m0, l0 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXNfqTtdePrZ","outputId":"2e057a56-03ed-45aa-919b-b45391e857b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.49440676], shape=(1,), dtype=float32)  Lambda =  0.2892324  Train_Loss : 0.016379257664084435, Val_Loss : 0.019611740484833717, Test_Loss : 0.16909007728099823\n","\n","\n"," Violation =  tf.Tensor([0.40509504], shape=(1,), dtype=float32)  Lambda =  0.22615786  Train_Loss : 0.1483832448720932, Val_Loss : 0.04130925238132477, Test_Loss : 0.2407130002975464\n","\n","\n"," Violation =  tf.Tensor([0.06619643], shape=(1,), dtype=float32)  Lambda =  -4.9222274  Train_Loss : 0.08554207533597946, Val_Loss : 0.03207075968384743, Test_Loss : 0.1961236447095871\n","\n","\n"," Violation =  tf.Tensor([0.03391527], shape=(1,), dtype=float32)  Lambda =  -4.160192  Train_Loss : 0.05694304034113884, Val_Loss : 0.02920548990368843, Test_Loss : 0.18136821687221527\n","\n","\n"," Violation =  tf.Tensor([0.00270282], shape=(1,), dtype=float32)  Lambda =  -3.9523466  Train_Loss : 0.046752117574214935, Val_Loss : 0.028081873431801796, Test_Loss : 0.17654967308044434\n","Validation Loss : 0.028081873431801796, Training Loss : 0.046752117574214935, Testing Loss : 0.17654967308044434\n"]}]},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, m0, l0 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"id":"NY64q_swL--5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27892d28-8764-4c4f-f32c-0defcdad94f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.49439812], shape=(1,), dtype=float32)  Lambda =  0.28912997  Train_Loss : 0.016379427164793015, Val_Loss : 0.019611859694123268, Test_Loss : 0.16908898949623108\n","\n","\n"," Violation =  tf.Tensor([0.29424903], shape=(1,), dtype=float32)  Lambda =  1.3477703  Train_Loss : 0.1025828942656517, Val_Loss : 0.03368252515792847, Test_Loss : 0.2100342959165573\n","\n","\n"," Violation =  tf.Tensor([0.00484691], shape=(1,), dtype=float32)  Lambda =  -3.5372188  Train_Loss : 0.06342198699712753, Val_Loss : 0.028537597507238388, Test_Loss : 0.1852010041475296\n","\n","\n"," Violation =  tf.Tensor([0.01933116], shape=(1,), dtype=float32)  Lambda =  -3.5505824  Train_Loss : 0.060265492647886276, Val_Loss : 0.030327534303069115, Test_Loss : 0.18351085484027863\n","\n","\n"," Violation =  tf.Tensor([0.02214812], shape=(1,), dtype=float32)  Lambda =  -1.7875507  Train_Loss : 0.09944881498813629, Val_Loss : 0.033670440316200256, Test_Loss : 0.2031596302986145\n","Validation Loss : 0.028537597507238388, Training Loss : 0.06342198699712753, Testing Loss : 0.1852010041475296\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"vwQ4cE0B9wXc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"GBHO_10K_1HP_.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}