{"cells":[{"cell_type":"code","execution_count":null,"id":"ceda8cd2","metadata":{"id":"ceda8cd2"},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn import preprocessing\n","from keras.utils.np_utils import to_categorical\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"id":"951d09f2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"951d09f2","outputId":"80bfeef1-8a03-4a6b-ab03-d0c3ef62a5fc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((600, 784), (600,), (400, 784), (400,), (10010, 784), (10010,))"]},"metadata":{},"execution_count":69}],"source":["train_dataset = np.load('train_dataset_1k_60.npz')  \n","val_dataset = np.load('validation_dataset_1k_40.npz')\n","test_dataset = np.load('test_dataset.npz')\n","\n","# =========== Loading Datasets ===============\n","\n","x_train = train_dataset['x'].reshape(600, 784).astype(\"float32\") / 255\n","y_train = train_dataset['y'].astype(\"float32\")\n","  \n","x_val = val_dataset['x'].reshape(400, 784).astype(\"float32\") / 255\n","y_val = val_dataset['y'].astype(\"float32\")   \n","                    \n","x_test = test_dataset['x'].reshape(10010, 784).astype(\"float32\") / 255\n","y_test = test_dataset['y'].astype(\"float32\")                    \n","\n","\n","x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape "]},{"cell_type":"code","execution_count":null,"id":"afefa986","metadata":{"id":"afefa986"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(784,)),\n","    tf.keras.layers.Dense(100, activation='relu'),\n","    tf.keras.layers.Dense(10)    # didn't use softmax since it will be called when (logits=true) in below step\n","])\n","loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"]},{"cell_type":"code","execution_count":null,"id":"cef9ecb3","metadata":{"id":"cef9ecb3"},"outputs":[],"source":["@tf.function\n","def loss(w1,w2,lamda,loss_fn,y_train,logits): # Lambda\n","    total_loss = loss_fn(y_train,logits)\n","    return total_loss + tf.math.exp(lamda)*(tf.nn.l2_loss(w1)+tf.nn.l2_loss(w2))/(2*y_train.shape[0])\n"]},{"cell_type":"markdown","id":"ba4b12ef","metadata":{"id":"ba4b12ef"},"source":["#### $\\phi(\\lambda) = min\\{l(w;S^T) + \\exp{\\lambda}(\\|w\\|_2^2)\\}$ for given $\\lambda$"]},{"cell_type":"code","execution_count":null,"id":"48045099","metadata":{"id":"48045099"},"outputs":[],"source":["wt_layer1_init = model.layers[1].get_weights()\n","wt_layer2_init = model.layers[2].get_weights()\n","\n","train_df = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","train_df = train_df.shuffle(buffer_size = 1024).batch(64)\n","\n","def fmin_loss(lamda, epochs= 50, l_rate= 0.6, momentum= 0.0599, nesterov = True):      # lamda, not exp(lamda), Works with both tf.Variable and tf.constant type lambda input, (or just scalar)\n","    tf.keras.backend.clear_session()\n","    optimizer = keras.optimizers.SGD(learning_rate=l_rate,momentum = momentum , nesterov =nesterov )\n","    total_loss0 = 1e20\n","    for epoch in range(epochs):\n","        for step,(x_train,y_train) in enumerate(train_df):\n","            with tf.GradientTape() as tape:\n","                logits = model(x_train, training=True)\n","                w1 = model.layers[1].weights[0]\n","                w2 = model.layers[2].weights[0]\n","                total_loss1 = loss(w1,w2,lamda,loss_fn,y_train,logits)      \n","            vars_list = model.trainable_weights\n","            grads = tape.gradient(total_loss1, vars_list)      # for ref  - https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough \n","            optimizer.apply_gradients(zip(grads,vars_list))   \n","        total_loss0 = total_loss1\n","        \n","    wt_layer1 = model.layers[1].get_weights()\n","    wt_layer2 = model.layers[2].get_weights()\n","    model.layers[1].set_weights(wt_layer1_init)\n","    model.layers[2].set_weights(wt_layer2_init)\n","\n","    return [total_loss1, wt_layer1, wt_layer2]"]},{"cell_type":"code","execution_count":null,"id":"c176e652","metadata":{"id":"c176e652"},"outputs":[],"source":["def f_val(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_train)\n","    return loss(w1,w2,lamda,loss_fn,y_train,logits)\n","\n","def f_grad(w1,b1,w2,b2,lamda): # lamda -> Variable\n","    with tf.GradientTape() as tape:\n","        total_loss = f_val(w1,b1,w2,b2,lamda)\n","    vars_list = model.trainable_weights\n","    vars_list.append(lamda)\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads\n","\n","def F_val(w1,b1,w2,b2):     \n","    layer1_weights = [w1,b1]\n","    layer2_weights = [w2,b2]\n","    model.layers[1].set_weights(layer1_weights)\n","    model.layers[2].set_weights(layer2_weights)\n","    logits = model(x_val)\n","    return loss_fn(y_val,logits)\n","\n","def F_grad(w1,b1,w2,b2):   # float32 arrays\n","    with tf.GradientTape() as tape:\n","        total_loss = F_val(w1,b1,w2,b2)\n","    vars_list = model.trainable_weights\n","    grads = tape.gradient(total_loss, vars_list) \n","    return grads"]},{"cell_type":"markdown","id":"e33981fc","metadata":{"id":"e33981fc"},"source":["### Gaussian Process Regressor"]},{"cell_type":"code","execution_count":null,"id":"f994b19c","metadata":{"id":"f994b19c"},"outputs":[],"source":["from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.gaussian_process.kernels import RBF\n","import matplotlib.pyplot as plt\n","\n","def GPR( Lamda_sample, Phi_sample, lamda ):\n","\n","    kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-4, 1e4))\n","    gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n","    gaussian_process.fit(Lamda_sample, Phi_sample)\n","\n","    k2_l = gaussian_process.kernel_.get_params()['k2__length_scale']\n","    x = lamda\n","    y_pred, sigma = gaussian_process.predict(x, return_std=True)\n","    y_pred_grad = 0.0*y_pred\n","\n","    for key, x_star in enumerate(x):\n","\n","        k_val=gaussian_process.kernel_( Lamda_sample , np.atleast_2d(x_star) , eval_gradient=False).ravel() \n","        x_diff_over_l_sq = ((Lamda_sample-x_star)/np.power(k2_l,2)).ravel()\n","        intermediate_result = np.multiply(k_val, x_diff_over_l_sq)\n","        final_result = np.dot(intermediate_result, gaussian_process.alpha_)\n","        y_pred_grad[key] = final_result\n","        \n","    return [y_pred[0], y_pred_grad[0]]"]},{"cell_type":"code","execution_count":null,"id":"5394a874","metadata":{"id":"5394a874"},"outputs":[],"source":["def Function_Grad( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","\n","    Phi = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    Phi_val, Phi_grad = Phi[0], Phi[1]\n","    \n","    p1 = F_grad( w1,b1,w2,b2 ) \n","    p2 = ( R_val* ( f_val(w1,b1,w2,b2,lamda) -  Phi_val) + Mu_val )\n","    f_gradient = f_grad(w1,b1,w2,b2,lamda)\n","    p3 = f_gradient[:-1] + [f_gradient[-1]-Phi_grad]\n","    p2tp3 = [p2*elt for elt in p3]\n","\n","    gradients = [e1+e2 for e1,e2 in zip(p1,p2tp3[:-1])] + [p2tp3[-1]]\n","    gradients = [(tf.clip_by_norm(grad, clip_norm = 2.0)) for grad in gradients]\n","\n","    return gradients   # Output : w1, b1, w2, b2, lamda : floar32 arrays"]},{"cell_type":"code","execution_count":null,"id":"9afbbbe3","metadata":{"id":"9afbbbe3"},"outputs":[],"source":["def AugLag_Function( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val ):\n","    PHI = GPR(Lamda_sample, Phi_sample, np.array([[lamda.numpy()[0]]]))\n","    f_phi = f_val(w1,b1,w2,b2,lamda) - PHI[0]\n","    final_val = F_val(w1,b1,w2,b2) + (R_val/2) * f_phi**2 + Mu_val * f_phi\n","    return final_val.numpy()[0]"]},{"cell_type":"code","execution_count":null,"id":"10c95548","metadata":{"id":"10c95548"},"outputs":[],"source":["def Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val, Mu_val, momentum, learning_rate, \n","                     epochs = 40, Opt_tol = 1e-6, F_tol = 1e-6 ): # With Nesterov Momentum\n","\n","    w1_update0 = np.zeros((784,100),dtype='float32')\n","    b1_update0 = np.zeros((100),dtype='float32')\n","    w2_update0 = np.zeros((100,10),dtype='float32')\n","    b2_update0 = np.zeros((10),dtype='float32')\n","    lamda_update0 = np.zeros((1),dtype='float32')\n","\n","    w1_weight0, w2_weight0, b1_weight0, b2_weight0, lamda_weight0 = w1, w2, b1, b2, lamda  # Initialized weights\n","    Loss_val = 1e+20\n","    Z_VAL0 = 1e+20\n","    #Opt_weights = []\n","\n","    Z_plot = []\n","    \n","    for epoch in range(epochs):\n","\n","        w1_weight_ahead = w1_weight0 - momentum*w1_update0\n","        b1_weight_ahead = b1_weight0 - momentum*b1_update0\n","        w2_weight_ahead = w2_weight0 - momentum*w2_update0\n","        b2_weight_ahead = b2_weight0 - momentum*b2_update0\n","\n","        lamda_weight_ahead = lamda_weight0 - momentum*lamda_update0\n","        lamda_weight_ahead = tf.Variable(lamda_weight_ahead.numpy(), dtype = tf.float32)\n","\n","        Grad_list = Function_Grad( w1_weight_ahead, b1_weight_ahead, w2_weight_ahead, b2_weight_ahead, \n","                                  lamda_weight_ahead, Lamda_sample, Phi_sample, R_val, Mu_val )\n","\n","        learning_rate = tf.constant(learning_rate, dtype=tf.float32)\n","        momentum = tf.constant(momentum, dtype=tf.float32)\n","\n","        part1 = [momentum*update for update in [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0]]\n","        part2 = [learning_rate * grad for grad in Grad_list]\n","\n","        [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0] = [i+j for i,j in zip(part1,part2)]  \n","        [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0] = np.subtract([w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0], \n","                                                                                      [w1_update0, b1_update0, w2_update0, b2_update0, lamda_update0])\n","        \n","        Z_VAL = AugLag_Function( w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0,\n","                                Lamda_sample, Phi_sample, R_val, Mu_val )\n","        Z_plot.append(Z_VAL)\n","        grad_tol = np.array([np.linalg.norm( optgd, ord = np.inf ) for optgd in Grad_list])\n","        fun_tol = abs( (Z_VAL-Loss_val)/(1+abs(Loss_val)) )\n","        Loss_val = Z_VAL\n","\n","        if np.all(grad_tol) <= Opt_tol and fun_tol <= F_tol:\n","          break\n","     \n","        #=====================================================================================\n","\n","    return [w1_weight0, b1_weight0, w2_weight0, b2_weight0, lamda_weight0], Loss_val   # w1, b1, w2, b2, lamda\n"]},{"cell_type":"code","source":["def LOSS_FUNCTION( wts1, wts2 ):\n","  \n","  model.layers[1].set_weights(wts1)\n","  model.layers[2].set_weights(wts2)\n","\n","  val_logits = model(x_val)\n","  val_loss = loss_fn(y_val,val_logits)\n","\n","  train_logits = model(x_train)\n","  training_loss = loss_fn(y_train,train_logits)\n","\n","  test_logits = model(x_test)\n","  test_loss = loss_fn(y_test,test_logits)\n","\n","  return val_loss, training_loss, test_loss"],"metadata":{"id":"p8S4vE8d0AIb"},"id":"p8S4vE8d0AIb","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"1bad8b81","metadata":{"id":"1bad8b81"},"outputs":[],"source":["# GLOBAL : Lamda_sample, Phi_sample\n","def Augmented_Lagrangian( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, mom, lr, R_val = 2,\n","                         Mu_val = 2, neta = 1.5, al_epochs = 5):\n","    Z_val0 = 1e+20\n","    wts_opt0, lamda_opt0, min_violation = [], 0, 10\n","    for epoch in range(al_epochs):\n","\n","        Opt_Weights, _ = Gradient_Descent( w1,b1,w2,b2,lamda, Lamda_sample, Phi_sample, R_val, Mu_val, mom, lr)\n","        \n","        w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt = Opt_Weights[0], Opt_Weights[1], Opt_Weights[2], Opt_Weights[3], Opt_Weights[4]  # lamda not var\n","        \n","        valL, trL, testL = LOSS_FUNCTION( [w1_opt,b1_opt], [w2_opt,b2_opt] )\n","\n","        # ========== Setting Weights for Next Loop ====================\n","        #w1,b1,w2,b2,lamda = w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt\n","      \n","        # ==========UPDATING LAMBDA SAMPLE FOR GPR=========================\n","        Lamda_sample = np.sort(np.array(np.vstack([Lamda_sample,[lamda_opt.numpy()]]), dtype = 'float32'), axis = 0)\n","        index = np.where(Lamda_sample == [lamda_opt.numpy()[0]] )\n","        Phi_new_lamda = fmin_loss(lamda_opt)[0].numpy()[0]\n","        Phi_sample = np.insert(Phi_sample, index[0][0], Phi_new_lamda)\n","\n","        # ============== Updating Augmented Lagrangian Parameters ====================\n","        Phi1 = GPR(Lamda_sample, Phi_sample, np.array([[lamda_opt.numpy()[0]]]))\n","        Phi_val1 = Phi1[0]\n","        lamda_opt = tf.Variable(lamda_opt, dtype=tf.float32)                   # Converting lamda to var\n","        constraint_ = f_val(w1_opt,b1_opt,w2_opt,b2_opt,lamda_opt) - Phi_new_lamda\n","        Mu_val = Mu_val + R_val*( constraint_ )\n","        R_val  = neta*R_val\n","\n","         # ===== Constraint Violation Criteria ===================\n","        violation = abs(constraint_)\n","        print(\"\\n\\n Violation = \", violation, \"Lambda = \", lamda_opt.numpy()[0], f\" Train_Loss : {trL}, Val_Loss : {valL}, Test_Loss : {testL}\")\n","        if violation <= min_violation:\n","          wts_opt0 = [ [w1_opt, b1_opt], [w2_opt, b2_opt]]\n","          lamda_opt0 = lamda_opt\n","          min_violation = violation\n","      \n","    print(f\"\\n Optimal Lambda = {lamda_opt0.numpy()[0]}\")\n","    return wts_opt0, lamda_opt0 #, min_violation"]},{"cell_type":"code","execution_count":null,"id":"6b1c0e6b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6b1c0e6b","outputId":"7ecb341b-b2ae-4d0e-bee7-3f89fa9cfaba"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Phi_sample:  [0.00298521 0.00186084 0.00209035 0.00384037 0.00929772 0.02414524\n"," 0.05921821 0.12800297 0.21853586 0.44237474] \n","\n","\n"]}],"source":["Lamda_sample = np.arange(-10,0,1, dtype = 'float32').reshape((-1,1))\n","\n","Phi_sample, Weights, lamda_init = [], [], 0\n","init_val_loss = 1e20\n","for lamda in Lamda_sample:\n","    \n","    [min_loss, layer1wt, layer2wt] = fmin_loss(lamda)\n","    Phi_sample.append( min_loss.numpy()[0] )\n","    \n","    model.layers[1].set_weights(layer1wt)\n","    model.layers[2].set_weights(layer2wt)\n","    val_logits = model(x_val)\n","    val_loss = loss_fn(y_val,val_logits)\n","    if val_loss <= init_val_loss :\n","        init_val_loss = val_loss\n","        Weights = [layer1wt, layer2wt]\n","        lamda_init = lamda\n","        \n","Phi_sample = np.array(Phi_sample)\n","print(\"\\n\\n Phi_sample: \", Phi_sample, \"\\n\\n\")\n","\n","#============================== Initializing Weights =================================================\n","# w1, b1 = Weights[0][0], Weights[0][1]\n","# w2, b2 = Weights[1][0], Weights[1][1]\n","# lamda = tf.Variable(lamda_init, dtype = tf.float32)\n"]},{"cell_type":"code","execution_count":null,"id":"5ab7ab14","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ab7ab14","outputId":"224388a6-c3a9-4d38-ce9b-0ea05b1a02db"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," LR : 0.0010000000474974513, MOM : 0.1, Z_VAL : 0.0007415927248075604\n","\n"," LR : 0.0010000000474974513, MOM : 0.5, Z_VAL : 0.0007265430176630616\n","\n"," LR : 0.0010000000474974513, MOM : 0.8, Z_VAL : 0.0006790617480874062\n","\n"," LR : 0.0010000000474974513, MOM : 0.9, Z_VAL : 0.0006103147752583027\n","\n"," LR : 0.0010000000474974513, MOM : 0.99, Z_VAL : 0.0002543895388953388\n","\n"," LR : 0.009999999776482582, MOM : 0.1, Z_VAL : 0.0005747901741415262\n","\n"," LR : 0.009999999776482582, MOM : 0.5, Z_VAL : 0.0004403999773785472\n","\n"," LR : 0.009999999776482582, MOM : 0.8, Z_VAL : 6.0157792177051306e-05\n","\n"," LR : 0.009999999776482582, MOM : 0.9, Z_VAL : -0.0004001309862360358\n","\n"," LR : 0.009999999776482582, MOM : 0.99, Z_VAL : -0.0017800162313506007\n","\n"," LR : 0.05000000074505806, MOM : 0.1, Z_VAL : -3.1066243536770344e-05\n","\n"," LR : 0.05000000074505806, MOM : 0.5, Z_VAL : -0.0004677737597376108\n","\n"," LR : 0.05000000074505806, MOM : 0.8, Z_VAL : -0.0013067718828096986\n","\n"," LR : 0.05000000074505806, MOM : 0.9, Z_VAL : -0.0018435853999108076\n","\n"," LR : 0.05000000074505806, MOM : 0.99, Z_VAL : -0.001403749454766512\n","\n"," LR : 0.07000000029802322, MOM : 0.1, Z_VAL : -0.00026970880571752787\n","\n"," LR : 0.07000000029802322, MOM : 0.5, Z_VAL : -0.0007724157767370343\n","\n"," LR : 0.07000000029802322, MOM : 0.8, Z_VAL : -0.0015970331151038408\n","\n"," LR : 0.07000000029802322, MOM : 0.9, Z_VAL : -0.0020039251539856195\n","\n"," LR : 0.07000000029802322, MOM : 0.99, Z_VAL : -0.0012482494348660111\n","\n"," LR : 0.10000000149011612, MOM : 0.1, Z_VAL : -0.0005680912872776389\n","\n"," LR : 0.10000000149011612, MOM : 0.5, Z_VAL : -0.001112513942644\n","\n"," LR : 0.10000000149011612, MOM : 0.8, Z_VAL : -0.0018390323966741562\n","\n"," LR : 0.10000000149011612, MOM : 0.9, Z_VAL : -0.0021012963261455297\n","\n"," LR : 0.10000000149011612, MOM : 0.99, Z_VAL : -0.0013443517964333296\n","\n","\n"," Optimum :  -0.0021012963  Momentum : 0.9, LR : 0.10000000149011612\n"]}],"source":["# learn_rate = np.arange( 0.1, 1, 0.1, dtype = 'float32' )\n","# momentum_set = np.arange( 0.01, 0.1, 0.01 )\n","learn_rate = np.array( [0.001, 0.01, 0.05, 0.07, 0.1], dtype = 'float32' )\n","momentum_set = np.array( [0.1, 0.5, 0.8, 0.9, 0.99] )\n","\n","opt_val = 1e20\n","l0,m0 = 0,0\n","for lr in learn_rate:\n","    for mom in momentum_set:\n","        w1, b1 = Weights[0][0], Weights[0][1]\n","        w2, b2 = Weights[1][0], Weights[1][1]\n","        lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","        wt, val = Gradient_Descent( w1,b1,w2,b2,lamda,Lamda_sample, Phi_sample, R_val = 2, Mu_val = 2,\n","                                   momentum = mom, learning_rate = lr )\n","        print(\"\\n LR : {}, MOM : {}, Z_VAL : {}\".format(lr,mom,val))\n","        \n","        if val <= opt_val:\n","            opt_val = val\n","            l0,m0 = lr,mom\n","print(\"\\n\\n Optimum : \",  opt_val, \" Momentum : {}, LR : {}\".format(m0,l0))"]},{"cell_type":"markdown","source":["## LLO = 10(+5)"],"metadata":{"id":"2YVLfkGO-s4j"},"id":"2YVLfkGO-s4j"},{"cell_type":"markdown","source":["###### Note : The result might differ a little because of GPR estimation every time the program is executed. Training the program to its optimal is avoided because it fits the model over training and validation dataset too well, leading to overfitting on validation dataset. "],"metadata":{"id":"sG1viENW-r_f"},"id":"sG1viENW-r_f"},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, 0.9, 0.1 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pvsq8jvWsR_7","outputId":"6315188e-e16a-4a33-fe68-dac391bf71f7"},"id":"Pvsq8jvWsR_7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.6483066], shape=(1,), dtype=float32) Lambda =  -0.13544567  Train_Loss : 0.014715105295181274, Val_Loss : 0.026634035632014275, Test_Loss : 0.3786086440086365\n","\n","\n"," Violation =  tf.Tensor([0.21309051], shape=(1,), dtype=float32) Lambda =  -1.505659  Train_Loss : 0.05297679454088211, Val_Loss : 0.026524653658270836, Test_Loss : 0.3883810043334961\n","\n","\n"," Violation =  tf.Tensor([0.04326952], shape=(1,), dtype=float32) Lambda =  -2.5588424  Train_Loss : 0.15922246873378754, Val_Loss : 0.027615830302238464, Test_Loss : 0.4309144914150238\n","\n","\n"," Violation =  tf.Tensor([0.01770394], shape=(1,), dtype=float32) Lambda =  -3.0946574  Train_Loss : 0.09684845060110092, Val_Loss : 0.027278058230876923, Test_Loss : 0.4036986827850342\n","\n","\n"," Violation =  tf.Tensor([0.01666894], shape=(1,), dtype=float32) Lambda =  -6.606596  Train_Loss : 0.02099953591823578, Val_Loss : 0.027952423319220543, Test_Loss : 0.37342071533203125\n","\n"," Optimal Lambda = -6.606595993041992\n","Validation Loss : 0.027952423319220543, Training Loss : 0.02099953591823578, Testing Loss : 0.37342071533203125\n"]}]},{"cell_type":"markdown","id":"5a92bc86","metadata":{"id":"5a92bc86"},"source":["### RUN 2"]},{"cell_type":"code","source":["# ===========FINDING LOSS=======================\n","\n","w1, b1 = Weights[0][0], Weights[0][1]\n","w2, b2 = Weights[1][0], Weights[1][1]\n","lamda = tf.Variable(lamda_init, dtype = tf.float32)\n","\n","wt_final, lamda_final = Augmented_Lagrangian( w1, b1, w2, b2, lamda, Lamda_sample, Phi_sample, m0, l0 )\n","\n","Final_weights_1, Final_weights_2 = wt_final[0], wt_final[1]\n","val_loss, training_loss, test_loss = LOSS_FUNCTION(Final_weights_1,Final_weights_2 )\n","\n","print(f'Validation Loss : {val_loss}, Training Loss : {training_loss}, Testing Loss : {test_loss}')"],"metadata":{"id":"DENA9MCyJKJm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c80480e-ed5b-405d-e0ef-b1840050434b"},"id":"DENA9MCyJKJm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Violation =  tf.Tensor([0.6131201], shape=(1,), dtype=float32) Lambda =  -0.13544904  Train_Loss : 0.014714931137859821, Val_Loss : 0.026634039357304573, Test_Loss : 0.37860870361328125\n","\n","\n"," Violation =  tf.Tensor([0.19829614], shape=(1,), dtype=float32) Lambda =  -1.3735832  Train_Loss : 0.05493297055363655, Val_Loss : 0.026477951556444168, Test_Loss : 0.39001980423927307\n","\n","\n"," Violation =  tf.Tensor([0.06693812], shape=(1,), dtype=float32) Lambda =  -1.6699228  Train_Loss : 0.16002924740314484, Val_Loss : 0.027685485780239105, Test_Loss : 0.4314601421356201\n","\n","\n"," Violation =  tf.Tensor([0.04734012], shape=(1,), dtype=float32) Lambda =  -2.9386547  Train_Loss : 0.13637197017669678, Val_Loss : 0.029343752190470695, Test_Loss : 0.4270420968532562\n","\n","\n"," Violation =  tf.Tensor([0.01415216], shape=(1,), dtype=float32) Lambda =  -2.9580843  Train_Loss : 0.08050461113452911, Val_Loss : 0.026997040957212448, Test_Loss : 0.3980245590209961\n","\n"," Optimal Lambda = -2.9580843448638916\n","Validation Loss : 0.026997040957212448, Training Loss : 0.08050461113452911, Testing Loss : 0.3980245590209961\n"]}]}],"metadata":{"accelerator":"TPU","colab":{"background_execution":"on","collapsed_sections":[],"name":"GBHO_1K_1HP_.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}